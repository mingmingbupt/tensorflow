{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "tf_basic_api.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mingmingbupt/tensorflow/blob/master/tf_basic_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuR6VtLt6DeT",
        "colab_type": "code",
        "outputId": "4642b44b-a35a-46fd-9eca-76b36cb8686b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "    print(module.__name__, module.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.2.0-rc1\n",
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
            "matplotlib 3.2.1\n",
            "numpy 1.18.2\n",
            "pandas 0.25.3\n",
            "sklearn 0.22.2.post1\n",
            "tensorflow 2.2.0-rc1\n",
            "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLDlGHmg6Dec",
        "colab_type": "code",
        "outputId": "69c843d1-9f11-4f6f-d317-01f77fa4bd6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "t = tf.constant([[1., 2., 3.], [4., 5., 6.]]) # 常量，这个函数就定义好了一个常量，用一个二维矩阵去初始化它\n",
        "\n",
        "# index 这是我们在常量上做的索引操作\n",
        "print(t) # 然后我们可以打印它\n",
        "print(t[:, 1:]) #也可以有很多操作，向numpy那样，比如说我要取出第二列以后的数值\n",
        "print(t[..., 1]) #比如说我要把第二列单独取出来，作为一个单独的tensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[2. 3.]\n",
            " [5. 6.]], shape=(2, 2), dtype=float32)\n",
            "tf.Tensor([2. 5.], shape=(2,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNW0-7xj6Deg",
        "colab_type": "code",
        "outputId": "2b644c8e-710d-4c5a-a0f8-040216cd7773",
        "colab": {}
      },
      "source": [
        "# ops 在常量上做的算子操作,基本上tensorflow的所有的算子在这个constant上都会支持，\n",
        "# 比如说加法，比如说平方，比如说矩阵乘以自己的转置tf.transpose呢就是做自己的转置\n",
        "print(t+10)\n",
        "print(tf.square(t))\n",
        "print(t @ tf.transpose(t)) # 矩阵乘以自己的转置tf.transpose呢就是做自己的转置\n",
        "# 其实呢，我们可以发现，已经用到了tensorflow2.0的新feature，也就是eager execution \n",
        "# 在1.0里面呢，如果我定义了一个常量，我调用他的print函数，是无法获得他具体的值的。因为我需要用session才能获取他具体的值\n",
        "# 而在2.0里面呢，我可以定义完了以后呢，print完了以后呢，就可以获得他的值了。因为他的eager execution是默认打开的。\n",
        "# 对应的，上面的op操作在1.0里面是需要在session里面去run一下，才能获取对应的值\n",
        "# 而在tensorflow2.0里面呢，这些值是可以直接获得到的，就是你做完操作以后，直接可以获取到"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[11. 12. 13.]\n",
            " [14. 15. 16.]], shape=(2, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 1.  4.  9.]\n",
            " [16. 25. 36.]], shape=(2, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[14. 32.]\n",
            " [32. 77.]], shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6EmyAtj6Dej",
        "colab_type": "code",
        "outputId": "854857f2-bcd3-4e26-dded-6647f430d2ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# numpy conversion\n",
        "# 在tensorflow2.0里面呢，还添加了tensorflow和numpy的转化操作\n",
        "print(t.numpy()) # 比如说我在tensorflow里定义的一个常量，我可以直接用t.numpy方法获取到他的值\n",
        "print(np.square(t)) # 同样的，对于一个tensor，我也可以把它作为一些numpy方法的输入。比如说一个tensor可以作为np.square的输入\n",
        "np_t = np.array([[1., 2., 3.], [4., 5., 6.]]) # 我可以把一个numpy的对象呢，直接转成tensor\n",
        "print(tf.constant(np_t)) # 我可以把一个numpy的对象呢，直接转成tensor，我们只需要在tf.constant里面传入这个numpy的数组就可以啦\n",
        "# 如果在一个tensorflow tensor上面直接调用numpy，会直接获取他的值\n",
        "# 如果是把一个tensor输入给numpy的一个函数呢，我们就会获取他算出来之后的那个值\n",
        "# 如果我们把numpy传给tf.constant，我们就会获取一个tensor对象\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "[[ 1.  4.  9.]\n",
            " [16. 25. 36.]]\n",
            "tf.Tensor(\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]], shape=(2, 3), dtype=float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1U1GZRk6Den",
        "colab_type": "code",
        "outputId": "877c186a-65fb-4daa-e04b-c7ade8830fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# Scalars \n",
        "# tensor可以是零维的，一维的，二维的，三维的，以至于n维的。 如果是零维的，怎么样个使用方法，\n",
        "# 零维其实就是对应一个数，然后在tensorflow中成为scalar.如果我要建立一个0维的tensor的话，直接用tf.constant(数字)，\n",
        "# 他的输入就是一个具体的数字就好了\n",
        "t = tf.constant(2.718)\n",
        "print(t.numpy())\n",
        "print(t.shape) # 0维"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.718\n",
            "()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxGZtqjG6Deq",
        "colab_type": "code",
        "outputId": "40c8c890-5c94-4523-e552-16cc837e7975",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# strings\n",
        "# 在tensorflow中一个tensor不仅可以存储数字，还可以存储字符串\n",
        "# 接下来看下如何和字符串做处理的。\n",
        "t = tf.constant(\"cafe\") # 输入是一个字符串\n",
        "print(t)\n",
        "print(tf.strings.length(t)) # 可以通过tf.string的相应函数，对这个字符串进行操作，\n",
        "print(tf.strings.length(t, unit=\"UTF8_CHAR\")) # 获取他utf编码时候的长度\n",
        "print(tf.strings.unicode_decode(t, \"UTF8\")) # 我们使用unicode_decode来把我们字符串呢，从unicode呢转化为utf-8"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'cafe', shape=(), dtype=string)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor([ 99  97 102 101], shape=(4,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfKIYZSk6Dev",
        "colab_type": "code",
        "outputId": "0b3b620d-f398-4aab-80f3-994f0c14e4cb",
        "colab": {}
      },
      "source": [
        "# string array\n",
        "t = tf.constant([\"cafe\", \"coffee\", \"咖啡\"]) # 除了一个字符串外，还可以存储一个数组的字符串\n",
        "print(tf.strings.length(t, unit=\"UTF8_CHAR\")) # 看下数组中每一个字符串的长度，这样就三个长度，\n",
        "r = tf.strings.unicode_decode(t, \"UTF8\") # 将unicode 转成utf8\n",
        "print(r) # 什么是RaggedTensor呢，就是不完整对象。因为每行的列数不完全一样，第二维长度不一样，这是tensorflow2.0里新加入的，是不规则的数据"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([4 6 2], shape=(3,), dtype=int32)\n",
            "<tf.RaggedTensor [[99, 97, 102, 101], [99, 111, 102, 102, 101, 101], [21654, 21857]]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lpIVf7w6Dey",
        "colab_type": "code",
        "outputId": "954b66ff-2d6d-461f-edb1-02e75e0a9821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# ragged tensor\n",
        "r = tf.ragged.constant([[11, 12], [21, 22, 23], [], [41]]) #我们使用tf.ragged来初始化他，使用不规则数据\n",
        "# index op\n",
        "print(r)  # ragged tensor\n",
        "print(r[1]) # 只取一行是个tensor\n",
        "print(r[1:2]) # 第一行到第二行，左闭右开区间，是个ragged tensor"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.RaggedTensor [[11, 12], [21, 22, 23], [], [41]]>\n",
            "tf.Tensor([21 22 23], shape=(3,), dtype=int32)\n",
            "<tf.RaggedTensor [[21, 22, 23]]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9leNKHl6De1",
        "colab_type": "code",
        "outputId": "ff2264e0-e369-4672-d580-b13d43dc32f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# ops on ragged tensor\n",
        "r2 = tf.ragged.constant([[51, 52], [], [71]]) \n",
        "print(tf.concat([r, r2], axis = 0)) # ragged tensor支持拼接操作， r1和r2进行拼接，他们是在第0维度上进行拼接，效果就是按照行的方式拼接起来，拼接完就是7行"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.RaggedTensor [[11, 12], [21, 22, 23], [], [41], [51, 52], [], [71]]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtekysgq6De5",
        "colab_type": "code",
        "outputId": "3db51421-b202-430d-c9a9-125a832ae99c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "r3 = tf.ragged.constant([[13, 14], [15], [], [42, 43]])\n",
        "print(tf.concat([r, r3], axis = 1)) # 在第一维度上进行拼接，需要第一个维度呢，两个在第一个维度上呢 是一样的。即有一样的行数"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.RaggedTensor [[11, 12, 13, 14], [21, 22, 23, 15], [], [41, 42, 43]]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDWK3R9o6De9",
        "colab_type": "code",
        "outputId": "7e228314-c576-4dd4-b1bf-ea33de9e1764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "print(r)\n",
        "print(r.to_tensor()) #regged tensor呢是可以转化为tensor的，只需要调to_tensor()函数就可以啦，缺少的数据用0补齐\n",
        "# 因为用0补齐，所以0都在正常值的后面"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.RaggedTensor [[11, 12], [21, 22, 23], [], [41]]>\n",
            "tf.Tensor(\n",
            "[[11 12  0]\n",
            " [21 22 23]\n",
            " [ 0  0  0]\n",
            " [41  0  0]], shape=(4, 3), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAP0FHz86DfA",
        "colab_type": "code",
        "outputId": "224840f5-ef82-4590-e7e8-ac012c2592db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "# sparse tensor\n",
        "s = tf.SparseTensor(indices = [[0, 1], [1, 0], [2, 3]], #第一个参数： 三个index, # 在一个非常大的二维矩阵里，只有几个值有数字，其他都为0，那么如何存储矩阵呢。\n",
        "                    values = [1., 2., 3.],#第二个参数： 这三个坐标位置的值 # 这种数据不适合用raggedTensor，因为raggedTensor填充值都在后面，\n",
        "                    dense_shape = [3, 4]) #第三个参数： 这个矩阵的具体的大小，比如是一个3*4的矩阵 # 可以用sparse tensor.在一个非常大的二维矩阵里，只有几个值有数字，其他都为0\n",
        "                                  # 那么我就把这些位置的坐标加上他的值 存储起来\n",
        "print(s)\n",
        "print(tf.sparse.to_dense(s)) #将sparetensor 转成普通的 tensor"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SparseTensor(indices=tf.Tensor(\n",
            "[[0 1]\n",
            " [1 0]\n",
            " [2 3]], shape=(3, 2), dtype=int64), values=tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n",
            "tf.Tensor(\n",
            "[[0. 1. 0. 0.]\n",
            " [2. 0. 0. 0.]\n",
            " [0. 0. 0. 3.]], shape=(3, 4), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAqm50_16DfD",
        "colab_type": "code",
        "outputId": "3b75e659-b997-4e13-ad5b-ff1f42f3b38e",
        "colab": {}
      },
      "source": [
        "# ops on sparse tensors\n",
        "\n",
        "s2 = s * 2.0 #sparse tensor 上的算子类型，可以做乘法\n",
        "print(s2)\n",
        "\n",
        "try:\n",
        "    s3 = s + 1  # 不支持加法， 加法有问题\n",
        "except TypeError as ex:\n",
        "    print(ex)\n",
        "\n",
        "s4 = tf.constant([[10., 20.],\n",
        "                  [30., 40.],\n",
        "                  [50., 60.],\n",
        "                  [70., 80.]]) #定义一个4*几的矩阵 这里是4*2的矩阵\n",
        "print(tf.sparse.sparse_dense_matmul(s, s4)) #s是一个3*4的矩阵，s跟s4相乘，得到的是一个普通3*2的 tensor\n",
        "\n",
        "#还有一个问题，在定义sparse tensor的时候 index必须是排好序的，如果没有排好序的话，虽然不影响sparse tensor的使用，但是如果调用toTense函数的话，他就会发生错误"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SparseTensor(indices=tf.Tensor(\n",
            "[[0 1]\n",
            " [1 0]\n",
            " [2 3]], shape=(3, 2), dtype=int64), values=tf.Tensor([2. 4. 6.], shape=(3,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n",
            "unsupported operand type(s) for +: 'SparseTensor' and 'int'\n",
            "tf.Tensor(\n",
            "[[ 30.  40.]\n",
            " [ 20.  40.]\n",
            " [210. 240.]], shape=(3, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwCBmZy86DfG",
        "colab_type": "code",
        "outputId": "074ffcd4-6119-451a-827f-c79ea23b5b06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "# sparse tensor\n",
        "s5 = tf.SparseTensor(indices = [[0, 2], [0, 1], [2, 3]], # 这里不让tensor Index排好序\n",
        "                    values = [1., 2., 3.],\n",
        "                    dense_shape = [3, 4])\n",
        "print(s5) # s5可以被打印出来\n",
        "#print(tf.sparse.to_dense(s5)) # 但是sparse_to_tenser的时候就会出错\n",
        "s6 = tf.sparse.reorder(s5)  # 我们需要对s5 reorder 就可以了\n",
        "print(tf.sparse.to_dense(s6)) # 在用s6调用spare_to_tense就没问题了 这是spase_to_tenser的一个比较常见的坑"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SparseTensor(indices=tf.Tensor(\n",
            "[[0 2]\n",
            " [0 1]\n",
            " [2 3]], shape=(3, 2), dtype=int64), values=tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n",
            "tf.Tensor(\n",
            "[[0. 2. 1. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 3.]], shape=(3, 4), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzx55Btc6DfJ",
        "colab_type": "code",
        "outputId": "f07e86ce-7061-4846-85da-d5e40b2fb019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "# Variables\n",
        "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]]) # 上面将的都是常量。在神经网络中常量是不会变得。但是实际我们得参数是要更新得，这时候就需要Variable\n",
        "print(v) # 一正常打印，他会获得一个tf.Variable对像\n",
        "print(v.value()) #把Variables变成tensor,这就跟普通的常量是一样的\n",
        "print(v.numpy()) #把Variables变成numpy，把具体的值打印出来"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[1., 2., 3.],\n",
            "       [4., 5., 6.]], dtype=float32)>\n",
            "tf.Tensor(\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjtskUN16DfR",
        "colab_type": "code",
        "outputId": "0bcf5fa7-cc84-414d-f9a1-5b335ba320a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# assign value  # 变量比常量多了一些操作，比如说重新赋值\n",
        "v.assign(2*v)    # 通过assign函数重新赋值\n",
        "print(v.numpy()) # 打印具体值\n",
        "v[0, 1].assign(42) #变量的某一个位置重新赋值\n",
        "print(v.numpy()) # 打印具体值\n",
        "v[1].assign([7., 8., 9.]) #变量的某一行重新赋值\n",
        "print(v.numpy()) # 打印具体值"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2.  4.  6.]\n",
            " [ 8. 10. 12.]]\n",
            "[[ 2. 42.  6.]\n",
            " [ 8. 10. 12.]]\n",
            "[[ 2. 42.  6.]\n",
            " [ 7.  8.  9.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLpg9RPG6DfU",
        "colab_type": "code",
        "outputId": "daae2136-4e72-4811-a434-ced2c0671381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "    v[1] = [7., 8., 9.] # tf.variable变量的赋值只能用assign函数，用等于号是不可以的\n",
        "except TypeError as ex:\n",
        "    print(ex)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'ResourceVariable' object does not support item assignment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTTx9xP1RA_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAL5tBjG6DfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}