{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "tf_keras_regression-customized_layer.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mingmingbupt/tensorflow/blob/master/tf_keras_regression_customized_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-hBv34YEMD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "117441a1-0a5b-44ad-ca47-16660073a60a"
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "    print(module.__name__, module.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc1\n",
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
            "matplotlib 3.2.1\n",
            "numpy 1.18.2\n",
            "pandas 0.25.3\n",
            "sklearn 0.22.2.post1\n",
            "tensorflow 2.2.0-rc1\n",
            "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuyHHLTJEMEB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c598b79-374b-4d49-f87e-72d27ac01a05"
      },
      "source": [
        "# Dense layer里面到底做了什么操作呢？\n",
        "layer = tf.keras.layers.Dense(100)  #这个100是这个全连接层的神经单元数目，\n",
        "# 一般我们会把tf.keras.layers.Dense(100) 放到Sequential里面去，或者model里面去，这是我们对dense layer的使用方法。\n",
        "# 我们使用dense layer的时候，一般也会指定另一个参数，也就是input_shpae,这个参数呢，它指定的是这个dense layer的输入\n",
        "# 我们需要他一般是在第一层的时候，也就是跟数据相连接的时候，我们一般会需要它。\n",
        "# 当然了，你不指定他也没关系，因为他在第一次运行的时候，会自动推导出来数据的size，然后给设置好input_shpae\n",
        "# 但是呢，如果你想明确指定他的话，一般情况下，我们就是在第一层的时候指定他，\n",
        "layer = tf.keras.layers.Dense(100, input_shape=(None, 5)) \n",
        "\n",
        "# 除了在Sequentital和model里使用dense layer呢，在tensor2.0里，对于这些layer呢， 都可以用像使用一个函数一样去使用它\n",
        "# 在tensorflow2.0里面有了eager mode，所有层次呢其实都可以像调用一个函数一样去调用它\n",
        "# 我们看下这个layer使用函数式调用\n",
        "# 这里我们只看第二个layer\n",
        "# 它的输入是样本数*5，这个None表示不定值\n",
        "# 他的输出是一个None*100的矩阵\n",
        "layer(tf.zeros([10, 5])) # 看下是否可以像调函数一样去调用它，这里输入是一个10*5的矩阵，对应的呢，这里的none就成了10，所以他的输出呢应该是一个10*100的矩阵\n",
        "#确实得到了一个10*100的矩阵，"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 100), dtype=float32, numpy=\n",
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wZ3muPfItq4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bbe70cef-f4dc-42c0-8bc5-e110f0fee27e"
      },
      "source": [
        "# 接下来我们看下，在layer里到底有什么样的东西\n",
        "# 其中layer 有很多很有用的方法，其中一个方法就是layer.variables\n",
        "layer.variables\n",
        "#我们看到输出有两部分，kernel和 bias\n",
        "#所谓的dense layer呢，其他他是一个全连接层。\n",
        "#比如说我的输入是x,dense layer做的事情呢我们用一个矩阵，也就是这里面的kernal乘以x然后呢，再加上bias\n",
        "#这样就是dense layer所做的事情。\n",
        "# layer.variables\n",
        "# x * w + b"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'dense_3/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
              " array([[ 0.16213135,  0.04339187, -0.03057945,  0.10188054,  0.12912937,\n",
              "          0.0089712 , -0.09927794,  0.18932275,  0.17290662, -0.05221225,\n",
              "         -0.15769206, -0.06225094,  0.17416622,  0.22201873,  0.1127174 ,\n",
              "          0.10552914, -0.02248588, -0.1126681 , -0.12962633, -0.03347874,\n",
              "         -0.14773762,  0.12251629, -0.03301926,  0.19482295, -0.15072107,\n",
              "         -0.15177408,  0.02915592,  0.22923307, -0.20157862, -0.05644627,\n",
              "         -0.0604898 , -0.23824224, -0.1214857 , -0.06090181,  0.05332609,\n",
              "         -0.01862375, -0.05088422, -0.0583276 , -0.22914782,  0.23612277,\n",
              "          0.16817226, -0.22506604, -0.02890442,  0.13878135, -0.16185369,\n",
              "          0.03401037, -0.09002452,  0.18756975,  0.07682921,  0.20113726,\n",
              "          0.17817782,  0.04042436, -0.02085274,  0.1067854 ,  0.20344798,\n",
              "         -0.00945473, -0.10824601,  0.02517514, -0.21749128, -0.1431992 ,\n",
              "         -0.20757712, -0.08742569, -0.1925783 , -0.15048353,  0.03568454,\n",
              "         -0.01057322, -0.16752854, -0.16348162, -0.08526611,  0.11382671,\n",
              "          0.01089221, -0.10876049,  0.1433209 , -0.15337   , -0.18681386,\n",
              "         -0.13410422,  0.1986437 ,  0.22637995,  0.07354997, -0.12796757,\n",
              "         -0.02920027,  0.22848369, -0.23576681,  0.03432076,  0.1891471 ,\n",
              "          0.00170454,  0.14637174, -0.18495703, -0.21231386,  0.06698568,\n",
              "         -0.14669868,  0.22777988,  0.08469893, -0.04716918,  0.11249752,\n",
              "          0.10970069,  0.16877304, -0.08383034,  0.18016084,  0.18707277],\n",
              "        [ 0.03102849, -0.05110505,  0.02058141,  0.11445419,  0.09312026,\n",
              "         -0.12081723,  0.02971669, -0.21653163, -0.13087392,  0.00615621,\n",
              "         -0.10931914, -0.16703138, -0.22674556, -0.13082616, -0.15418512,\n",
              "          0.00482713, -0.19994172, -0.17588285,  0.1483963 , -0.02363731,\n",
              "         -0.03080851,  0.12588818, -0.087975  ,  0.06549896,  0.1371706 ,\n",
              "         -0.08686721,  0.19046529,  0.18290974, -0.21363907,  0.21041088,\n",
              "          0.23717697, -0.08766414,  0.04912291, -0.17024612, -0.13673256,\n",
              "          0.11885537,  0.08415537, -0.20643716, -0.04578716, -0.01206923,\n",
              "          0.14974709, -0.10001971,  0.15357973, -0.04391973, -0.09406474,\n",
              "          0.0266393 ,  0.11741088,  0.03873532,  0.02147295,  0.1746649 ,\n",
              "         -0.23205583, -0.03235233,  0.13071491,  0.14737065,  0.10915022,\n",
              "         -0.19873747, -0.02050994, -0.03627492,  0.02814527,  0.14031719,\n",
              "         -0.02643184,  0.1694419 , -0.22064419, -0.02087224, -0.18009457,\n",
              "         -0.10186131,  0.2266032 ,  0.0262575 , -0.01018606, -0.11835068,\n",
              "          0.08419763,  0.16674824,  0.21782304,  0.10378431, -0.16608587,\n",
              "         -0.00456332,  0.1381091 , -0.04250453,  0.21206833,  0.19208775,\n",
              "         -0.12210687,  0.11764927,  0.21550028,  0.03723074, -0.2091446 ,\n",
              "          0.02785222, -0.02479991,  0.22968115, -0.10645661, -0.01646064,\n",
              "          0.20919482, -0.10684353,  0.02559473,  0.14746575, -0.19578774,\n",
              "          0.18298258, -0.17634523,  0.13653652, -0.20456466,  0.01186188],\n",
              "        [ 0.21055011,  0.2005205 ,  0.01244538, -0.05713469,  0.17167489,\n",
              "          0.18461536,  0.04503651,  0.09787427,  0.16457064, -0.08416831,\n",
              "         -0.00988041,  0.13568155, -0.09642373, -0.01720075,  0.05052163,\n",
              "         -0.03994687, -0.18920079,  0.04609971, -0.11683752, -0.0351418 ,\n",
              "         -0.1096583 ,  0.07125936, -0.03948483,  0.20324309,  0.09145595,\n",
              "          0.0038285 , -0.17931074, -0.1057279 , -0.06163439,  0.22273321,\n",
              "         -0.04087432, -0.08411343,  0.03973772, -0.23835497, -0.19157612,\n",
              "         -0.13686   , -0.00489421, -0.19084543,  0.10840501, -0.19612604,\n",
              "         -0.20878145,  0.04691835, -0.05229564,  0.18931203,  0.23210074,\n",
              "         -0.23827204,  0.01048146, -0.11761855, -0.04379766,  0.08485572,\n",
              "         -0.02020361, -0.0625086 , -0.07307795,  0.04045837,  0.15430947,\n",
              "          0.04745488,  0.16531195,  0.08822976,  0.02835159, -0.12478935,\n",
              "          0.18437211,  0.12623976,  0.15997799, -0.01041415, -0.1586459 ,\n",
              "          0.21792488,  0.2306657 ,  0.16195546, -0.21919423, -0.13362332,\n",
              "         -0.17094901,  0.13244931, -0.13642025, -0.01444019, -0.03347874,\n",
              "          0.18177955,  0.02792205,  0.16101624, -0.15920545,  0.03272249,\n",
              "          0.12363552,  0.16133301,  0.12300222,  0.21553852,  0.0683677 ,\n",
              "         -0.04542315,  0.00620459, -0.2246036 , -0.16132867, -0.10204262,\n",
              "          0.2368279 , -0.09402439, -0.23297785, -0.09194705,  0.14495538,\n",
              "         -0.13251588, -0.15329784, -0.14885992, -0.20205805,  0.10665818],\n",
              "        [-0.11975625,  0.17866485, -0.13372864, -0.01100607, -0.16155118,\n",
              "         -0.09038202, -0.23419705, -0.04824783,  0.13559447, -0.1870913 ,\n",
              "          0.04276688,  0.19075875,  0.11768325,  0.1174996 , -0.22782752,\n",
              "         -0.16326688,  0.16207258,  0.04330029, -0.15429722,  0.13698669,\n",
              "          0.15981255,  0.04982825, -0.11091687,  0.1212943 , -0.12180976,\n",
              "          0.09057428, -0.05168462,  0.17005543, -0.03161141, -0.09064944,\n",
              "          0.03036447,  0.05015688, -0.22667591,  0.02854384,  0.10809784,\n",
              "         -0.06478843, -0.14525944,  0.11918963, -0.1778862 ,  0.0576822 ,\n",
              "          0.14221089, -0.00988097, -0.20431274, -0.01605731, -0.20881918,\n",
              "          0.10435767, -0.14254037,  0.12288494,  0.07886808,  0.12848608,\n",
              "          0.19251104, -0.04240918,  0.22890656, -0.09394853,  0.22618447,\n",
              "          0.16529383,  0.04275627,  0.15131767,  0.11894365, -0.18469304,\n",
              "         -0.05918637, -0.0139582 , -0.0375351 ,  0.06338139, -0.08589788,\n",
              "         -0.0677637 ,  0.13192014, -0.21110483,  0.08487861, -0.10746709,\n",
              "         -0.07566212,  0.20423551, -0.02868216, -0.10369381, -0.16706473,\n",
              "          0.01238059,  0.06084989,  0.07045309, -0.08439338,  0.16689591,\n",
              "         -0.03475863,  0.15605573,  0.01665948, -0.13281527, -0.11482539,\n",
              "          0.21085463, -0.04468282, -0.16731817,  0.06404634, -0.11444604,\n",
              "         -0.10449958, -0.04113203,  0.07199098, -0.17472902, -0.02660356,\n",
              "         -0.17203331,  0.08727415, -0.1357333 , -0.16119349,  0.02473094],\n",
              "        [-0.02213264,  0.14493863, -0.07724738,  0.04129435, -0.04078113,\n",
              "         -0.09732366,  0.03284161, -0.05811809, -0.2112729 ,  0.22048359,\n",
              "          0.04246013, -0.16233921,  0.01430346,  0.1037154 ,  0.21752997,\n",
              "          0.10495688,  0.19955008, -0.2301135 ,  0.01527934,  0.18656634,\n",
              "          0.1185378 ,  0.00976762,  0.09503333, -0.01930977, -0.1740516 ,\n",
              "         -0.23111817,  0.00570357,  0.20672427,  0.03164743,  0.08939196,\n",
              "         -0.16639727,  0.22656278,  0.13036965, -0.02059263, -0.08308619,\n",
              "         -0.20434238, -0.12136043, -0.09039171,  0.15096115,  0.05120753,\n",
              "         -0.18388015, -0.06973074, -0.06026354,  0.06590663, -0.16257516,\n",
              "          0.10731561,  0.22874321,  0.15934332, -0.07298476, -0.00123236,\n",
              "         -0.14649141,  0.06035285,  0.20601381,  0.11592899,  0.03627212,\n",
              "         -0.19531669,  0.09989701,  0.22773986,  0.01378767,  0.03230102,\n",
              "          0.08403866, -0.14000966, -0.20268866,  0.02009217,  0.18218027,\n",
              "         -0.21774387, -0.1620796 , -0.10528307, -0.15788646, -0.14700235,\n",
              "          0.02928828, -0.01080871,  0.12730388, -0.18591508,  0.05643566,\n",
              "          0.03457294, -0.02577129, -0.08242626,  0.19617642, -0.05855592,\n",
              "         -0.07848065, -0.15012574,  0.11645021, -0.0557795 ,  0.22491004,\n",
              "          0.20210348,  0.10070749, -0.08147682,  0.11882134,  0.08053218,\n",
              "         -0.16153401, -0.05004591,  0.0858622 ,  0.10503094, -0.14118958,\n",
              "          0.03927274,  0.07027094, -0.23613356, -0.05385417,  0.04730032]],\n",
              "       dtype=float32)>,\n",
              " <tf.Variable 'dense_3/bias:0' shape=(100,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRkbAOvTEMED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bfa9c053-74f2-4149-ffc0-7c5eb6007264"
      },
      "source": [
        "# 另一个比较重要的参数就是trainable_variables，即所有可训练参数。也是kernel 和bias\n",
        "# x * w + b\n",
        "# kernel相当于w, bias相当于b\n",
        "# layer里面其他函数 可以通过help来进行查看，\n",
        "layer.trainable_variables\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'dense_3/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
              " array([[ 0.16213135,  0.04339187, -0.03057945,  0.10188054,  0.12912937,\n",
              "          0.0089712 , -0.09927794,  0.18932275,  0.17290662, -0.05221225,\n",
              "         -0.15769206, -0.06225094,  0.17416622,  0.22201873,  0.1127174 ,\n",
              "          0.10552914, -0.02248588, -0.1126681 , -0.12962633, -0.03347874,\n",
              "         -0.14773762,  0.12251629, -0.03301926,  0.19482295, -0.15072107,\n",
              "         -0.15177408,  0.02915592,  0.22923307, -0.20157862, -0.05644627,\n",
              "         -0.0604898 , -0.23824224, -0.1214857 , -0.06090181,  0.05332609,\n",
              "         -0.01862375, -0.05088422, -0.0583276 , -0.22914782,  0.23612277,\n",
              "          0.16817226, -0.22506604, -0.02890442,  0.13878135, -0.16185369,\n",
              "          0.03401037, -0.09002452,  0.18756975,  0.07682921,  0.20113726,\n",
              "          0.17817782,  0.04042436, -0.02085274,  0.1067854 ,  0.20344798,\n",
              "         -0.00945473, -0.10824601,  0.02517514, -0.21749128, -0.1431992 ,\n",
              "         -0.20757712, -0.08742569, -0.1925783 , -0.15048353,  0.03568454,\n",
              "         -0.01057322, -0.16752854, -0.16348162, -0.08526611,  0.11382671,\n",
              "          0.01089221, -0.10876049,  0.1433209 , -0.15337   , -0.18681386,\n",
              "         -0.13410422,  0.1986437 ,  0.22637995,  0.07354997, -0.12796757,\n",
              "         -0.02920027,  0.22848369, -0.23576681,  0.03432076,  0.1891471 ,\n",
              "          0.00170454,  0.14637174, -0.18495703, -0.21231386,  0.06698568,\n",
              "         -0.14669868,  0.22777988,  0.08469893, -0.04716918,  0.11249752,\n",
              "          0.10970069,  0.16877304, -0.08383034,  0.18016084,  0.18707277],\n",
              "        [ 0.03102849, -0.05110505,  0.02058141,  0.11445419,  0.09312026,\n",
              "         -0.12081723,  0.02971669, -0.21653163, -0.13087392,  0.00615621,\n",
              "         -0.10931914, -0.16703138, -0.22674556, -0.13082616, -0.15418512,\n",
              "          0.00482713, -0.19994172, -0.17588285,  0.1483963 , -0.02363731,\n",
              "         -0.03080851,  0.12588818, -0.087975  ,  0.06549896,  0.1371706 ,\n",
              "         -0.08686721,  0.19046529,  0.18290974, -0.21363907,  0.21041088,\n",
              "          0.23717697, -0.08766414,  0.04912291, -0.17024612, -0.13673256,\n",
              "          0.11885537,  0.08415537, -0.20643716, -0.04578716, -0.01206923,\n",
              "          0.14974709, -0.10001971,  0.15357973, -0.04391973, -0.09406474,\n",
              "          0.0266393 ,  0.11741088,  0.03873532,  0.02147295,  0.1746649 ,\n",
              "         -0.23205583, -0.03235233,  0.13071491,  0.14737065,  0.10915022,\n",
              "         -0.19873747, -0.02050994, -0.03627492,  0.02814527,  0.14031719,\n",
              "         -0.02643184,  0.1694419 , -0.22064419, -0.02087224, -0.18009457,\n",
              "         -0.10186131,  0.2266032 ,  0.0262575 , -0.01018606, -0.11835068,\n",
              "          0.08419763,  0.16674824,  0.21782304,  0.10378431, -0.16608587,\n",
              "         -0.00456332,  0.1381091 , -0.04250453,  0.21206833,  0.19208775,\n",
              "         -0.12210687,  0.11764927,  0.21550028,  0.03723074, -0.2091446 ,\n",
              "          0.02785222, -0.02479991,  0.22968115, -0.10645661, -0.01646064,\n",
              "          0.20919482, -0.10684353,  0.02559473,  0.14746575, -0.19578774,\n",
              "          0.18298258, -0.17634523,  0.13653652, -0.20456466,  0.01186188],\n",
              "        [ 0.21055011,  0.2005205 ,  0.01244538, -0.05713469,  0.17167489,\n",
              "          0.18461536,  0.04503651,  0.09787427,  0.16457064, -0.08416831,\n",
              "         -0.00988041,  0.13568155, -0.09642373, -0.01720075,  0.05052163,\n",
              "         -0.03994687, -0.18920079,  0.04609971, -0.11683752, -0.0351418 ,\n",
              "         -0.1096583 ,  0.07125936, -0.03948483,  0.20324309,  0.09145595,\n",
              "          0.0038285 , -0.17931074, -0.1057279 , -0.06163439,  0.22273321,\n",
              "         -0.04087432, -0.08411343,  0.03973772, -0.23835497, -0.19157612,\n",
              "         -0.13686   , -0.00489421, -0.19084543,  0.10840501, -0.19612604,\n",
              "         -0.20878145,  0.04691835, -0.05229564,  0.18931203,  0.23210074,\n",
              "         -0.23827204,  0.01048146, -0.11761855, -0.04379766,  0.08485572,\n",
              "         -0.02020361, -0.0625086 , -0.07307795,  0.04045837,  0.15430947,\n",
              "          0.04745488,  0.16531195,  0.08822976,  0.02835159, -0.12478935,\n",
              "          0.18437211,  0.12623976,  0.15997799, -0.01041415, -0.1586459 ,\n",
              "          0.21792488,  0.2306657 ,  0.16195546, -0.21919423, -0.13362332,\n",
              "         -0.17094901,  0.13244931, -0.13642025, -0.01444019, -0.03347874,\n",
              "          0.18177955,  0.02792205,  0.16101624, -0.15920545,  0.03272249,\n",
              "          0.12363552,  0.16133301,  0.12300222,  0.21553852,  0.0683677 ,\n",
              "         -0.04542315,  0.00620459, -0.2246036 , -0.16132867, -0.10204262,\n",
              "          0.2368279 , -0.09402439, -0.23297785, -0.09194705,  0.14495538,\n",
              "         -0.13251588, -0.15329784, -0.14885992, -0.20205805,  0.10665818],\n",
              "        [-0.11975625,  0.17866485, -0.13372864, -0.01100607, -0.16155118,\n",
              "         -0.09038202, -0.23419705, -0.04824783,  0.13559447, -0.1870913 ,\n",
              "          0.04276688,  0.19075875,  0.11768325,  0.1174996 , -0.22782752,\n",
              "         -0.16326688,  0.16207258,  0.04330029, -0.15429722,  0.13698669,\n",
              "          0.15981255,  0.04982825, -0.11091687,  0.1212943 , -0.12180976,\n",
              "          0.09057428, -0.05168462,  0.17005543, -0.03161141, -0.09064944,\n",
              "          0.03036447,  0.05015688, -0.22667591,  0.02854384,  0.10809784,\n",
              "         -0.06478843, -0.14525944,  0.11918963, -0.1778862 ,  0.0576822 ,\n",
              "          0.14221089, -0.00988097, -0.20431274, -0.01605731, -0.20881918,\n",
              "          0.10435767, -0.14254037,  0.12288494,  0.07886808,  0.12848608,\n",
              "          0.19251104, -0.04240918,  0.22890656, -0.09394853,  0.22618447,\n",
              "          0.16529383,  0.04275627,  0.15131767,  0.11894365, -0.18469304,\n",
              "         -0.05918637, -0.0139582 , -0.0375351 ,  0.06338139, -0.08589788,\n",
              "         -0.0677637 ,  0.13192014, -0.21110483,  0.08487861, -0.10746709,\n",
              "         -0.07566212,  0.20423551, -0.02868216, -0.10369381, -0.16706473,\n",
              "          0.01238059,  0.06084989,  0.07045309, -0.08439338,  0.16689591,\n",
              "         -0.03475863,  0.15605573,  0.01665948, -0.13281527, -0.11482539,\n",
              "          0.21085463, -0.04468282, -0.16731817,  0.06404634, -0.11444604,\n",
              "         -0.10449958, -0.04113203,  0.07199098, -0.17472902, -0.02660356,\n",
              "         -0.17203331,  0.08727415, -0.1357333 , -0.16119349,  0.02473094],\n",
              "        [-0.02213264,  0.14493863, -0.07724738,  0.04129435, -0.04078113,\n",
              "         -0.09732366,  0.03284161, -0.05811809, -0.2112729 ,  0.22048359,\n",
              "          0.04246013, -0.16233921,  0.01430346,  0.1037154 ,  0.21752997,\n",
              "          0.10495688,  0.19955008, -0.2301135 ,  0.01527934,  0.18656634,\n",
              "          0.1185378 ,  0.00976762,  0.09503333, -0.01930977, -0.1740516 ,\n",
              "         -0.23111817,  0.00570357,  0.20672427,  0.03164743,  0.08939196,\n",
              "         -0.16639727,  0.22656278,  0.13036965, -0.02059263, -0.08308619,\n",
              "         -0.20434238, -0.12136043, -0.09039171,  0.15096115,  0.05120753,\n",
              "         -0.18388015, -0.06973074, -0.06026354,  0.06590663, -0.16257516,\n",
              "          0.10731561,  0.22874321,  0.15934332, -0.07298476, -0.00123236,\n",
              "         -0.14649141,  0.06035285,  0.20601381,  0.11592899,  0.03627212,\n",
              "         -0.19531669,  0.09989701,  0.22773986,  0.01378767,  0.03230102,\n",
              "          0.08403866, -0.14000966, -0.20268866,  0.02009217,  0.18218027,\n",
              "         -0.21774387, -0.1620796 , -0.10528307, -0.15788646, -0.14700235,\n",
              "          0.02928828, -0.01080871,  0.12730388, -0.18591508,  0.05643566,\n",
              "          0.03457294, -0.02577129, -0.08242626,  0.19617642, -0.05855592,\n",
              "         -0.07848065, -0.15012574,  0.11645021, -0.0557795 ,  0.22491004,\n",
              "          0.20210348,  0.10070749, -0.08147682,  0.11882134,  0.08053218,\n",
              "         -0.16153401, -0.05004591,  0.0858622 ,  0.10503094, -0.14118958,\n",
              "          0.03927274,  0.07027094, -0.23613356, -0.05385417,  0.04730032]],\n",
              "       dtype=float32)>,\n",
              " <tf.Variable 'dense_3/bias:0' shape=(100,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK5xzrQ5KCU5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a6e744a-07a4-467c-9acb-a21db7f94868"
      },
      "source": [
        "help(layer) # 通过help 可以看到layer里的其他方法"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on Dense in module tensorflow.python.keras.layers.core object:\n",
            "\n",
            "class Dense(tensorflow.python.keras.engine.base_layer.Layer)\n",
            " |  Just your regular densely-connected NN layer.\n",
            " |  \n",
            " |  `Dense` implements the operation:\n",
            " |  `output = activation(dot(input, kernel) + bias)`\n",
            " |  where `activation` is the element-wise activation function\n",
            " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
            " |  created by the layer, and `bias` is a bias vector created by the layer\n",
            " |  (only applicable if `use_bias` is `True`).\n",
            " |  \n",
            " |  Note: If the input to the layer has a rank greater than 2, then `Dense`\n",
            " |  computes the dot product between the `inputs` and the `kernel` along the\n",
            " |  last axis of the `inputs` and axis 1 of the `kernel` (using `tf.tensordot`).\n",
            " |  For example, if input has dimensions `(batch_size, d0, d1)`,\n",
            " |  then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates\n",
            " |  along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)`\n",
            " |  (there are `batch_size * d0` such sub-tensors).\n",
            " |  The output in this case will have shape `(batch_size, d0, units)`.\n",
            " |  \n",
            " |  Besides, layer attributes cannot be modified after the layer has been called\n",
            " |  once (except the `trainable` attribute).\n",
            " |  \n",
            " |  Example:\n",
            " |  \n",
            " |  ```python\n",
            " |  # as first layer in a sequential model:\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32, input_shape=(16,)))\n",
            " |  # now the model will take as input arrays of shape (*, 16)\n",
            " |  # and output arrays of shape (*, 32)\n",
            " |  \n",
            " |  # after the first layer, you don't need to specify\n",
            " |  # the size of the input anymore:\n",
            " |  model.add(Dense(32))\n",
            " |  ```\n",
            " |  \n",
            " |  Arguments:\n",
            " |    units: Positive integer, dimensionality of the output space.\n",
            " |    activation: Activation function to use.\n",
            " |      If you don't specify anything, no activation is applied\n",
            " |      (ie. \"linear\" activation: `a(x) = x`).\n",
            " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
            " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
            " |    bias_initializer: Initializer for the bias vector.\n",
            " |    kernel_regularizer: Regularizer function applied to\n",
            " |      the `kernel` weights matrix.\n",
            " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
            " |    activity_regularizer: Regularizer function applied to\n",
            " |      the output of the layer (its \"activation\")..\n",
            " |    kernel_constraint: Constraint function applied to\n",
            " |      the `kernel` weights matrix.\n",
            " |    bias_constraint: Constraint function applied to the bias vector.\n",
            " |  \n",
            " |  Input shape:\n",
            " |    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
            " |    The most common situation would be\n",
            " |    a 2D input with shape `(batch_size, input_dim)`.\n",
            " |  \n",
            " |  Output shape:\n",
            " |    N-D tensor with shape: `(batch_size, ..., units)`.\n",
            " |    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
            " |    the output would have shape `(batch_size, units)`.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Dense\n",
            " |      tensorflow.python.keras.engine.base_layer.Layer\n",
            " |      tensorflow.python.module.module.Module\n",
            " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
            " |      tensorflow.python.training.tracking.base.Trackable\n",
            " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
            " |  \n",
            " |  build(self, input_shape)\n",
            " |      Creates the variables of the layer (optional, for subclass implementers).\n",
            " |      \n",
            " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
            " |      can override if they need a state-creation step in-between\n",
            " |      layer instantiation and layer call.\n",
            " |      \n",
            " |      This is typically used to create the weights of `Layer` subclasses.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
            " |          `TensorShape` if the layer expects a list of inputs\n",
            " |          (one instance per input).\n",
            " |  \n",
            " |  call(self, inputs)\n",
            " |      This is where the layer's logic lives.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: Input tensor, or list/tuple of input tensors.\n",
            " |          **kwargs: Additional keyword arguments.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor or list/tuple of tensors.\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      If the layer has not been built, this method will call `build` on the\n",
            " |      layer. This assumes that the layer will later be used with inputs that\n",
            " |      match the input shape provided here.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An input shape tuple.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __call__(self, *args, **kwargs)\n",
            " |      Wraps `call`, applying pre- and post-processing steps.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        *args: Positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |      \n",
            " |      Note:\n",
            " |        - The following optional keyword arguments are reserved for specific uses:\n",
            " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
            " |            whether the `call` is meant for training or inference.\n",
            " |          * `mask`: Boolean input mask.\n",
            " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
            " |          layers do), its default value will be set to the mask generated\n",
            " |          for `inputs` by the previous layer (if `input` did come from\n",
            " |          a layer that generated a corresponding mask, i.e. if it came from\n",
            " |          a Keras layer with masking support.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
            " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_loss(self, losses, inputs=None)\n",
            " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Some losses (for instance, activity regularization losses) may be dependent\n",
            " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
            " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
            " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This method can be used inside a subclassed layer or model's `call`\n",
            " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyLayer(tf.keras.layers.Layer):\n",
            " |        def call(inputs, self):\n",
            " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any loss Tensors passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      losses become part of the model's topology and are tracked in `get_config`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Activity regularization.\n",
            " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      ```\n",
            " |      \n",
            " |      If this is not the case for your loss (if, for example, your loss references\n",
            " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
            " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
            " |      topology since they can't be serialized.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Weight regularization.\n",
            " |      model.add_loss(lambda: tf.reduce_mean(x.kernel))\n",
            " |      ```\n",
            " |      \n",
            " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
            " |      specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
            " |          may also be zero-argument callables which create a loss tensor.\n",
            " |        inputs: Ignored when executing eagerly. If anything other than None is\n",
            " |          passed, it signals the losses are conditional on some of the layer's\n",
            " |          inputs, and thus they should only be run where these inputs are\n",
            " |          available. This is the case for activity regularization losses, for\n",
            " |          instance. If `None` is passed, the losses are assumed\n",
            " |          to be unconditional, and will apply across all dataflows of the layer\n",
            " |          (e.g. weight regularization losses).\n",
            " |  \n",
            " |  add_metric(self, value, aggregation=None, name=None)\n",
            " |      Adds metric tensor to the layer.\n",
            " |      \n",
            " |      Args:\n",
            " |        value: Metric tensor.\n",
            " |        aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n",
            " |          it indicates that the metric tensor provided has been aggregated\n",
            " |          already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n",
            " |          `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n",
            " |          given metric tensor will be sample-wise reduced using `mean` function.\n",
            " |          eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n",
            " |          aggregation='mean')`.\n",
            " |        name: String metric name.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: If `aggregation` is anything other than None or `mean`.\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
            " |      \n",
            " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
            " |      Instructions for updating:\n",
            " |      `inputs` is now automatically inferred\n",
            " |      \n",
            " |      Weight updates (for instance, the updates of the moving mean and variance\n",
            " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
            " |      when calling a layer. Hence, when reusing the same layer on\n",
            " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
            " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
            " |      specific set of inputs.\n",
            " |      \n",
            " |      This call is ignored when eager execution is enabled (in that case, variable\n",
            " |      updates are run on the fly and thus do not need to be tracked for later\n",
            " |      execution).\n",
            " |      \n",
            " |      Arguments:\n",
            " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
            " |          that returns an update op. A zero-arg callable should be passed in\n",
            " |          order to disable running the updates by setting `trainable=False`\n",
            " |          on this Layer, when executing in Eager mode.\n",
            " |        inputs: Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_variable(self, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
            " |      \n",
            " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
            " |      Instructions for updating:\n",
            " |      Please use `layer.add_weight` method instead.\n",
            " |  \n",
            " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
            " |      Adds a new variable to the layer.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        name: Variable name.\n",
            " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
            " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
            " |        initializer: Initializer instance (callable).\n",
            " |        regularizer: Regularizer instance (callable).\n",
            " |        trainable: Boolean, whether the variable should be part of the layer's\n",
            " |          \"trainable_variables\" (e.g. variables, biases)\n",
            " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
            " |          Note that `trainable` cannot be `True` if `synchronization`\n",
            " |          is set to `ON_READ`.\n",
            " |        constraint: Constraint instance (callable).\n",
            " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
            " |        use_resource: Whether to use `ResourceVariable`.\n",
            " |        synchronization: Indicates when a distributed a variable will be\n",
            " |          aggregated. Accepted values are constants defined in the class\n",
            " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
            " |          `AUTO` and the current `DistributionStrategy` chooses\n",
            " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
            " |          `trainable` must not be set to `True`.\n",
            " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
            " |          Accepted values are constants defined in the class\n",
            " |          `tf.VariableAggregation`.\n",
            " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
            " |          `collections`, `experimental_autocast` and `caching_device`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
            " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
            " |        instance is returned.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called with partitioned variable regularization and\n",
            " |          eager execution is enabled.\n",
            " |        ValueError: When giving unsupported dtype and no initializer or when\n",
            " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
            " |  \n",
            " |  apply(self, inputs, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! (deprecated)\n",
            " |      \n",
            " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
            " |      Instructions for updating:\n",
            " |      Please use `layer.__call__` method instead.\n",
            " |      \n",
            " |      This is an alias of `self.__call__`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor(s).\n",
            " |        *args: additional positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask=None)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_signature(self, input_signature)\n",
            " |      Compute the output tensor signature of the layer based on the inputs.\n",
            " |      \n",
            " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
            " |      and dtype information for a tensor. This method allows layers to provide\n",
            " |      output dtype information if it is different from the input dtype.\n",
            " |      For any layer that doesn't implement this function,\n",
            " |      the framework will fall back to use `compute_output_shape`, and will\n",
            " |      assume that the output dtype matches the input dtype.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
            " |          objects, describing a candidate input for the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
            " |          how the layer would transform the provided input.\n",
            " |      \n",
            " |      Raises:\n",
            " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Count the total number of scalars composing the weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An integer count.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if the layer isn't yet built\n",
            " |            (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_losses_for(self, inputs)\n",
            " |      Retrieves losses relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of loss tensors of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |      Retrieves updates relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of update ops of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Returns the current weights of the layer.\n",
            " |      \n",
            " |      The weights of a layer represent the state of the layer. This function\n",
            " |      returns both trainable and non-trainable weight values associated with this\n",
            " |      layer as a list of Numpy arrays, which can in turn be used to load state\n",
            " |      into similarly parameterized layers.\n",
            " |      \n",
            " |      For example, a Dense layer returns a list of two values-- per-output\n",
            " |      weights and the bias value. These can be used to set the weights of another\n",
            " |      Dense layer:\n",
            " |      \n",
            " |      >>> a = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
            " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
            " |      >>> a.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
            " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[2.],\n",
            " |             [2.],\n",
            " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b.set_weights(a.get_weights())\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      \n",
            " |      Returns:\n",
            " |          Weights values as a list of numpy arrays.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the layer, from Numpy arrays.\n",
            " |      \n",
            " |      The weights of a layer represent the state of the layer. This function\n",
            " |      sets the weight values from numpy arrays. The weight values should be\n",
            " |      passed in the order they are created by the layer. Note that the layer's\n",
            " |      weights must be instantiated before calling this function by calling\n",
            " |      the layer.\n",
            " |      \n",
            " |      For example, a Dense layer returns a list of two values-- per-output\n",
            " |      weights and the bias value. These can be used to set the weights of another\n",
            " |      Dense layer:\n",
            " |      \n",
            " |      >>> a = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
            " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
            " |      >>> a.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
            " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[2.],\n",
            " |             [2.],\n",
            " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b.set_weights(a.get_weights())\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      \n",
            " |      Arguments:\n",
            " |          weights: a list of Numpy arrays. The number\n",
            " |              of arrays and their shape must match\n",
            " |              number of the dimensions of the weights\n",
            " |              of the layer (i.e. it should match the\n",
            " |              output of `get_weights`).\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If the provided weights list does not match the\n",
            " |              layer's specifications.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  from_config(config) from builtins.type\n",
            " |      Creates a layer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same layer from the config\n",
            " |      dictionary. It does not handle layer connectivity\n",
            " |      (handled by Network), nor weights (handled by `set_weights`).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          config: A Python dictionary, typically the\n",
            " |              output of get_config.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  activity_regularizer\n",
            " |      Optional regularizer function for the output of this layer.\n",
            " |  \n",
            " |  dtype\n",
            " |      Dtype used by the weights of the layer, set in the constructor.\n",
            " |  \n",
            " |  dynamic\n",
            " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
            " |  \n",
            " |  inbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |        AttributeError: If no inbound nodes are found.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
            " |      have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined input_shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  input_spec\n",
            " |      `InputSpec` instance(s) describing the input format for this layer.\n",
            " |      \n",
            " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
            " |      the layer to run input compatibility checks when it is called.\n",
            " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
            " |      of rank 4. As such, you can set, in `__init__()`:\n",
            " |      \n",
            " |      ```python\n",
            " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
            " |      ```\n",
            " |      \n",
            " |      Now, if you try to call the layer on an input that isn't rank 4\n",
            " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
            " |      error:\n",
            " |      \n",
            " |      ```\n",
            " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
            " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
            " |      ```\n",
            " |      \n",
            " |      Input checks that can be specified via `input_spec` include:\n",
            " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
            " |      - Shape\n",
            " |      - Rank (ndim)\n",
            " |      - Dtype\n",
            " |      \n",
            " |      For more information, see `tf.keras.layers.InputSpec`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
            " |  \n",
            " |  losses\n",
            " |      Losses which are associated with this `Layer`.\n",
            " |      \n",
            " |      Variable regularization tensors are created when this property is accessed,\n",
            " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
            " |      propagate gradients back to the corresponding variables.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of tensors.\n",
            " |  \n",
            " |  metrics\n",
            " |      List of `tf.keras.metrics.Metric` instances tracked by the layer.\n",
            " |  \n",
            " |  name\n",
            " |      Name of the layer (string), set in the constructor.\n",
            " |  \n",
            " |  non_trainable_variables\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |      List of all non-trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Non-trainable weights are *not* updated during training. They are expected\n",
            " |      to be updated manually in `call()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of non-trainable variables.\n",
            " |  \n",
            " |  outbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one output,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor or list of output tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        AttributeError: if the layer is connected to more than one incoming\n",
            " |          layers.\n",
            " |        RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one output,\n",
            " |      or if all outputs have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined output shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  stateful\n",
            " |  \n",
            " |  trainable\n",
            " |  \n",
            " |  trainable_variables\n",
            " |      Sequence of trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  trainable_weights\n",
            " |      List of all trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Trainable weights are updated via gradient descent during training.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of trainable variables.\n",
            " |  \n",
            " |  updates\n",
            " |  \n",
            " |  variables\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Alias of `self.weights`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  with_name_scope(method) from builtins.type\n",
            " |      Decorator to automatically enter the module name scope.\n",
            " |      \n",
            " |      >>> class MyModule(tf.Module):\n",
            " |      ...   @tf.Module.with_name_scope\n",
            " |      ...   def __call__(self, x):\n",
            " |      ...     if not hasattr(self, 'w'):\n",
            " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
            " |      ...     return tf.matmul(x, self.w)\n",
            " |      \n",
            " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
            " |      names included the module name:\n",
            " |      \n",
            " |      >>> mod = MyModule()\n",
            " |      >>> mod(tf.ones([1, 2]))\n",
            " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
            " |      >>> mod.w\n",
            " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
            " |      numpy=..., dtype=float32)>\n",
            " |      \n",
            " |      Args:\n",
            " |        method: The method to wrap.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The original method wrapped such that it enters the module's name scope.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  name_scope\n",
            " |      Returns a `tf.name_scope` instance for this class.\n",
            " |  \n",
            " |  submodules\n",
            " |      Sequence of all sub-modules.\n",
            " |      \n",
            " |      Submodules are modules which are properties of this module, or found as\n",
            " |      properties of modules which are properties of this module (and so on).\n",
            " |      \n",
            " |      >>> a = tf.Module()\n",
            " |      >>> b = tf.Module()\n",
            " |      >>> c = tf.Module()\n",
            " |      >>> a.b = b\n",
            " |      >>> b.c = c\n",
            " |      >>> list(a.submodules) == [b, c]\n",
            " |      True\n",
            " |      >>> list(b.submodules) == [c]\n",
            " |      True\n",
            " |      >>> list(c.submodules) == []\n",
            " |      True\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of all submodules.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from tensorflow.python.keras.utils.version_utils.LayerVersionSelector:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwargs)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NedmtekqEMEG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "outputId": "b6b86399-885f-46cd-b18f-ef575cbe3e7a"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "print(housing.DESCR)\n",
        "print(housing.data.shape)\n",
        "print(housing.target.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 20640\n",
            "\n",
            "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            "    :Attribute Information:\n",
            "        - MedInc        median income in block\n",
            "        - HouseAge      median house age in block\n",
            "        - AveRooms      average number of rooms\n",
            "        - AveBedrms     average number of bedrooms\n",
            "        - Population    block population\n",
            "        - AveOccup      average house occupancy\n",
            "        - Latitude      house block latitude\n",
            "        - Longitude     house block longitude\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "http://lib.stat.cmu.edu/datasets/\n",
            "\n",
            "The target variable is the median house value for California districts.\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "      Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n",
            "(20640, 8)\n",
            "(20640,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pyWQ5DREMEI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d743f34b-726f-41bb-d663-23312f6659ae"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
        "    housing.data, housing.target, random_state = 7)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(\n",
        "    x_train_all, y_train_all, random_state = 11)\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_valid.shape, y_valid.shape)\n",
        "print(x_test.shape, y_test.shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11610, 8) (11610,)\n",
            "(3870, 8) (3870,)\n",
            "(5160, 8) (5160,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8geV40ZLEMEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_valid_scaled = scaler.transform(x_valid)\n",
        "x_test_scaled = scaler.transform(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLgNMVEtEMEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d112baa2-49cd-40bc-e5d4-4e03616c26b7"
      },
      "source": [
        "# 下面是通过子类的方法完成了自定义的tensor layer的实现\n",
        "# 自定义的dense layer，我们需要定义一个子类\n",
        "# 实现了三个方法，三个方法分别实现了三个功能\n",
        "# 对于tensor lyaer这种带参数的layer呢，我们定义了一个子类，感觉还不是那么重\n",
        "# 如果我们只是像把要把一个随意的简单的函数定义为一个层次呢，如果他里面没有参数\n",
        "# 这样呢，这个build函数就完全没有必要了，然后我们整个类就比较小，也不是那么雅观\n",
        "# 然后呢，用子类实现也比较重，那么有没有比较简单的方式去构建自定义的层次呢\n",
        "# 对于那些没有参数的函数，我们想把它自定义成层次呢，可以使用lambda的方式来自定义层次\n",
        "# 下面演示如何使用lambda的方式来自定义层次\n",
        "\n",
        "# 定义一个激活函数，这个激活函数的表达式呢 tf.nn.softplus 的输入是x,输出是 log(1+e^x)\n",
        "# 他就相当于是一个平滑版的relu。relu在0的地方有一个折点的，这个是平滑的，是没有折点的\n",
        "# 我们看到这只是一个激活函数，并没有参数，我们想把它定义为一个layer的话，我们如果使用子类的话，就会写很多行代码\n",
        "# 如果使用lambda的话，只需要一行代码就可以了\n",
        "# 我们定义一个变量叫customized_softplus，它使用lambda来实现，这个lambda定义在keras,layers. \n",
        "# 在lamdda里传入一个lambda表达式，这样呢一个没有参数的softplus 的对应形式就产生了\n",
        "customized_softplus = keras.layers.Lambda(lambda x : tf.nn.softplus(x))\n",
        "#可以使用一下，传进去一个向量，\n",
        "print(customized_softplus([-10., -5., 0., 5., 10.]))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([4.5417706e-05 6.7153489e-03 6.9314718e-01 5.0067153e+00 1.0000046e+01], shape=(5,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNpBPs2DEMES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "707d1a56-a0eb-445f-821f-307248ffd8c7"
      },
      "source": [
        "# customized dense layer. \n",
        "# 在模型定义这里实现自定义的dense layer\n",
        "# 我们通过继承类的方式进行实现\n",
        "class CustomizedDenseLayer(keras.layers.Layer): # 我写个类，它继承自keras里面的keras.layers.Layer\n",
        "    # 在建立好子类的名字以后呢，我们就可以去重载里面的方法，从而实现自定义的效果\n",
        "    # 对于自定义layer呢，我们需要重载以下几个方法\n",
        "\n",
        "    # 一个就是初始化函数\n",
        "    def __init__(self, units, activation=None, **kwargs): \n",
        "        # 第一个参数是units，也就是这一层的输出有多少单元，这里呢，我们建立一个变量把他存起来\n",
        "        self.units = units\n",
        "\n",
        "        # 第二个参数是activation，也就是这一层用什么样的激活函数，我们也建立一个变量把他存起来\n",
        "        # 这里呢activation也是用layer来实现的\n",
        "        self.activation = keras.layers.Activation(activation)\n",
        "\n",
        "        # 接下来去调用父类的函数\n",
        "        super(CustomizedDenseLayer, self).__init__(**kwargs)\n",
        "        # 这样呢，我们的初始化函数就构造好了\n",
        "    \n",
        "    # 第二个呢，就是我们的build函数，build呢是负责所有的参数呢，都初始化起来\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"构建所需要的参数，一个是kernel,一个是bias,也就是w和b \"\"\"\n",
        "       \n",
        "        # 首先我们构建kernel参数，这里我们构建参数的时候，我们使用的是add_weight\n",
        "        # add_weight是它的父类layer里面的一个子方法,这个方法可以帮我们得到一个变量\n",
        "        # 下一个参数是shape，也就是这个kernel矩阵的大小\n",
        "        # x * w + b.\n",
        "        # 对于x来说的话，它的input_shape:[None, a] \n",
        "        # 我们的输出呢output_shape: [None, b]\n",
        "        # 那么这个矩阵w的大小呢，就是[a,b]。 即w:[a,b]\n",
        "        # 所以我们kernel的维度呢，就是input_shape得后面的值，和我们的输出值\n",
        "        # 接下来的一个参数initializer 他是用来决定如何初始化这个参数矩阵的，也就是kernel的初始化\n",
        "        # 我们可以使用uniform或者其他方式来初始化，即我们用均匀分布的方法去初始化我们的kernel\n",
        "        # 接下来我们去定义trainable，表示这个参数是可以可训练的。\n",
        "        # 如果是不能被训练，也就是这个参数在训练过程中是不变的话，那么这个trainable就设置成false\n",
        "        self.kernel = self.add_weight(name = 'kernel',\n",
        "                                      shape = (input_shape[1], self.units),\n",
        "                                      initializer = 'uniform',\n",
        "                                      trainable = True)\n",
        "        \n",
        "        # 接下来定义bias，同样使用add_weight函数来进行定义，它的名字是bias\n",
        "        # 它的shape呢，因为我们的w*x的值呢，已经是[none, b]的一个矩阵了\n",
        "        # 所以我们的bias呢就是一个长度为b的向量，\n",
        "        self.bias = self.add_weight(name = 'bias',\n",
        "                                    shape = (self.units, ),\n",
        "                                    initializer = 'zeros',\n",
        "                                    trainable = True)\n",
        "        # 同样要去调用父类的build函数\n",
        "        super(CustomizedDenseLayer, self).build(input_shape)\n",
        "    \n",
        "    # 第三个呢，就是我们的call函数，这个call函数呢，就是完成一次正向计算\n",
        "    # 在之前wide&deep中，跟这个比较像，不过在widedeep里面我们把init和build和到了一起\n",
        "    # 而在这里呢，是分开的\n",
        "    def call(self, x):\n",
        "        \"\"\"完成正向计算\"\"\"\n",
        "        # 就是如何从输入得到输出，这里x @ self.kernel，是x和kernel做矩阵乘法，再加上bias\n",
        "        # 即wx+b,再经过激活函数\n",
        "        return self.activation(x @ self.kernel + self.bias)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    #这里呢，我们就使用我们自定义的层次来构建模型\n",
        "    CustomizedDenseLayer(30, activation='relu',\n",
        "                         input_shape=x_train.shape[1:]),\n",
        "    #这里是一个数\n",
        "    CustomizedDenseLayer(1),\n",
        "    #一个激活函数层\n",
        "    customized_softplus,\n",
        "    # keras.layers.Dense(1, activation=\"softplus\"), 这个和customized_softplus是等价的\n",
        "    # keras.layers.Dense(1), keras.layers.Activation('softplus'), 或者用两层替换 customized_softplus是等价的\n",
        "])\n",
        "model.summary()\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "callbacks = [keras.callbacks.EarlyStopping(\n",
        "    patience=5, min_delta=1e-2)]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "customized_dense_layer_2 (Cu (None, 30)                270       \n",
            "_________________________________________________________________\n",
            "customized_dense_layer_3 (Cu (None, 1)                 31        \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 301\n",
            "Trainable params: 301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqiAgzMREMEV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "outputId": "7de9939d-0fc5-4f87-a2f8-0777f4210cd5"
      },
      "source": [
        "history = model.fit(x_train_scaled, y_train,\n",
        "                    validation_data = (x_valid_scaled, y_valid),\n",
        "                    epochs = 100,\n",
        "                    callbacks = callbacks)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 1.1658 - val_loss: 0.6666\n",
            "Epoch 2/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5843 - val_loss: 0.5816\n",
            "Epoch 3/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5148 - val_loss: 0.5213\n",
            "Epoch 4/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4738 - val_loss: 0.4866\n",
            "Epoch 5/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4603 - val_loss: 0.4801\n",
            "Epoch 6/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4655 - val_loss: 0.5020\n",
            "Epoch 7/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4476 - val_loss: 0.4513\n",
            "Epoch 8/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4323 - val_loss: 0.4652\n",
            "Epoch 9/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.4308\n",
            "Epoch 10/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4150 - val_loss: 0.4222\n",
            "Epoch 11/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4115 - val_loss: 0.4144\n",
            "Epoch 12/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3974 - val_loss: 0.4086\n",
            "Epoch 13/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3904 - val_loss: 0.4078\n",
            "Epoch 14/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3872 - val_loss: 0.3991\n",
            "Epoch 15/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3839 - val_loss: 0.3969\n",
            "Epoch 16/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3805 - val_loss: 0.3948\n",
            "Epoch 17/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3781 - val_loss: 0.3952\n",
            "Epoch 18/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3773 - val_loss: 0.3869\n",
            "Epoch 19/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3757 - val_loss: 0.3934\n",
            "Epoch 20/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3739 - val_loss: 0.3817\n",
            "Epoch 21/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3719 - val_loss: 0.3855\n",
            "Epoch 22/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3710 - val_loss: 0.3873\n",
            "Epoch 23/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3710 - val_loss: 0.3824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dSUyAvTEMEX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "e79cfd62-a3df-472b-9e4c-61fa96db03b8"
      },
      "source": [
        "def plot_learning_curves(history):\n",
        "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "    plt.grid(True)\n",
        "    plt.gca().set_ylim(0, 1)\n",
        "    plt.show()\n",
        "plot_learning_curves(history)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmMJYRETEY/8feCgz7lXPdj4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqJH2HTyEMEa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "03f24f76-abe5-492a-d6cb-4824ce34f508"
      },
      "source": [
        "model.evaluate(x_test_scaled, y_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "162/162 [==============================] - 0s 889us/step - loss: 0.3862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3861701488494873"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54xut4rWEMEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}