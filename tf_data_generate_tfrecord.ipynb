{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "tf_data_generate_tfrecord.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mingmingbupt/tensorflow/blob/master/tf_data_generate_tfrecord.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM9q9t3vC6NQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "94d9928d-309c-4ab7-e2fe-4226666f860e"
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "    print(module.__name__, module.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n",
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
            "matplotlib 3.2.1\n",
            "numpy 1.18.2\n",
            "pandas 1.0.3\n",
            "sklearn 0.22.2.post1\n",
            "tensorflow 2.2.0-rc2\n",
            "tensorflow.keras 2.3.0-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpmaeaYaIiCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "596468c8-faea-4367-c932-e95e9ab7fbf0"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing = fetch_california_housing()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0ethoQUI44z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b14af81c-88e9-411d-9f28-1f65395c7765"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
        "    housing.data, housing.target, random_state = 7)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(\n",
        "    x_train_all, y_train_all, random_state = 11)\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_valid.shape, y_valid.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11610, 8) (11610,)\n",
            "(3870, 8) (3870,)\n",
            "(5160, 8) (5160,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxlFUawYImv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_valid_scaled = scaler.transform(x_valid)\n",
        "x_test_scaled = scaler.transform(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z_M66ESAQfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#我们把scale后的数据呢，生产csv文件\n",
        "output_dir = \"generate_csv\" #定义一个文件夹，把生成的文件放到这个文件夹下\n",
        "if not os.path.exists(output_dir): #如果不存在文件夹的话，就用os.mkdir去创建这个文件夹\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "def save_to_csv(output_dir, data, name_prefix,\n",
        "                header=None, n_parts=10): #把一个单独的dataset,train/valid/test,保存到文件当中去\n",
        "                # output_dir： 输出文件夹，\n",
        "                # data ： dataset\n",
        "                # name_prefix： 因为我们要分别给train/valid/test分别生成csv文件，所以需要name_prefix去做区分\n",
        "                # header： 对于csv文件呢，可能需要一个header,这里默认是None\n",
        "                # n_parts: 我们的datesets是可以把多个文件去merge起来形成一个数据集的，\n",
        "                #      所以说呢，为了测试这个场景，我们需要把我们的数据集去切分成多个文件来进行存储，这里默认是10个文件\n",
        "    path_format = os.path.join(output_dir, \"{}_{:02d}.csv\") # 生成文件名 第一个{}里面天train,test还是valid. 第二个中括号填的是两位的一个整数\n",
        "    # 如果是一位的一个整数，就需要补全一位，这里填每个n_parts的具体数字\n",
        "    filenames = [] #最后会返回所有的文件名\n",
        "    \n",
        "    # enumerate就是给每一组标记一个值，也就是说每一组呢，我们可以通过这个row_indices获得，它标记的值呢，就是file_idx\n",
        "    # 就是一个整数，这个整数可以用来当成一个file_id\n",
        "    for file_idx, row_indices in enumerate(\n",
        "        #我用一个np.arange生成了一个和data一样长度的数组，也就是说我data里面有n个元素，那我们这个数组里面也有n个元素\n",
        "        #它的元素值就是0到n-1，用这个数值当索引。我们再把当索引的这个数值，分成了n_parts个部分\n",
        "        #有了这n_parts部分以后，用每一组里面的索引去data里面去取元素，从而获得数据\n",
        "        np.array_split(np.arange(len(data)), n_parts)):\n",
        "        part_csv = path_format.format(name_prefix, file_idx) #生成对应的子文件名\n",
        "        filenames.append(part_csv)\n",
        "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
        "            if header is not None:\n",
        "                f.write(header + \"\\n\")\n",
        "            for row_index in row_indices: #行索引\n",
        "                f.write(\",\".join(\n",
        "                    [repr(col) for col in data[row_index]])) #用逗号分割，需要做字符串化处理\n",
        "                f.write('\\n')\n",
        "    return filenames\n",
        "\n",
        "train_data = np.c_[x_train_scaled, y_train]  # np.c_把数据按行merge\n",
        "valid_data = np.c_[x_valid_scaled, y_valid]  \n",
        "test_data = np.c_[x_test_scaled, y_test]\n",
        "# 这样就把train valid test数据merge起来了\n",
        "# 然后生成header,从housing数据集获得，因为我们把y也拼接起来了，所以也要加个y的名字MidianHouseValue\n",
        "header_cols = housing.feature_names + [\"MidianHouseValue\"]\n",
        "header_str = \",\".join(header_cols) # 用逗号连接起来\n",
        "\n",
        "train_filenames = save_to_csv(output_dir, train_data, \"train\",\n",
        "                              header_str, n_parts=20) \n",
        "valid_filenames = save_to_csv(output_dir, valid_data, \"valid\",\n",
        "                              header_str, n_parts=10)\n",
        "test_filenames = save_to_csv(output_dir, test_data, \"test\",\n",
        "                             header_str, n_parts=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aagKL7gC6NV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "outputId": "36e9ae23-2eb4-47d0-f190-46d0b54b1ca0"
      },
      "source": [
        "#代码从这里开始\n",
        "source_dir = \"./generate_csv/\" #csv的文件夹放到这下面\n",
        "print(os.listdir(source_dir))\n",
        "# 首先获取train test valid 的文件list\n",
        "# 定义一个简单的函数来讲这40个文件进行区分，区分就是利用前缀进行划分\n",
        "def get_filenames_by_prefix(source_dir, prefix_name):\n",
        "    all_files = os.listdir(source_dir)\n",
        "    results = []\n",
        "    for filename in all_files:\n",
        "        if filename.startswith(prefix_name):\n",
        "            results.append(os.path.join(source_dir, filename))\n",
        "    return results\n",
        "\n",
        "train_filenames = get_filenames_by_prefix(source_dir, \"train\")\n",
        "valid_filenames = get_filenames_by_prefix(source_dir, \"valid\")\n",
        "test_filenames = get_filenames_by_prefix(source_dir, \"test\")\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(train_filenames)\n",
        "pprint.pprint(valid_filenames)\n",
        "pprint.pprint(test_filenames)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['train_08.csv', 'train_02.csv', 'test_01.csv', 'train_03.csv', 'train_09.csv', 'test_05.csv', 'test_03.csv', 'test_00.csv', 'valid_07.csv', 'train_15.csv', 'valid_03.csv', 'valid_00.csv', 'valid_08.csv', 'valid_09.csv', 'test_06.csv', 'train_16.csv', 'valid_02.csv', 'train_10.csv', 'test_04.csv', 'train_19.csv', 'train_17.csv', 'train_00.csv', 'train_13.csv', 'train_06.csv', 'train_07.csv', 'valid_05.csv', 'train_01.csv', 'train_14.csv', 'valid_06.csv', 'test_07.csv', 'test_02.csv', 'test_08.csv', 'valid_01.csv', 'test_09.csv', 'train_05.csv', 'train_18.csv', 'train_04.csv', 'valid_04.csv', 'train_11.csv', 'train_12.csv']\n",
            "['./generate_csv/train_08.csv',\n",
            " './generate_csv/train_02.csv',\n",
            " './generate_csv/train_03.csv',\n",
            " './generate_csv/train_09.csv',\n",
            " './generate_csv/train_15.csv',\n",
            " './generate_csv/train_16.csv',\n",
            " './generate_csv/train_10.csv',\n",
            " './generate_csv/train_19.csv',\n",
            " './generate_csv/train_17.csv',\n",
            " './generate_csv/train_00.csv',\n",
            " './generate_csv/train_13.csv',\n",
            " './generate_csv/train_06.csv',\n",
            " './generate_csv/train_07.csv',\n",
            " './generate_csv/train_01.csv',\n",
            " './generate_csv/train_14.csv',\n",
            " './generate_csv/train_05.csv',\n",
            " './generate_csv/train_18.csv',\n",
            " './generate_csv/train_04.csv',\n",
            " './generate_csv/train_11.csv',\n",
            " './generate_csv/train_12.csv']\n",
            "['./generate_csv/valid_07.csv',\n",
            " './generate_csv/valid_03.csv',\n",
            " './generate_csv/valid_00.csv',\n",
            " './generate_csv/valid_08.csv',\n",
            " './generate_csv/valid_09.csv',\n",
            " './generate_csv/valid_02.csv',\n",
            " './generate_csv/valid_05.csv',\n",
            " './generate_csv/valid_06.csv',\n",
            " './generate_csv/valid_01.csv',\n",
            " './generate_csv/valid_04.csv']\n",
            "['./generate_csv/test_01.csv',\n",
            " './generate_csv/test_05.csv',\n",
            " './generate_csv/test_03.csv',\n",
            " './generate_csv/test_00.csv',\n",
            " './generate_csv/test_06.csv',\n",
            " './generate_csv/test_04.csv',\n",
            " './generate_csv/test_07.csv',\n",
            " './generate_csv/test_02.csv',\n",
            " './generate_csv/test_08.csv',\n",
            " './generate_csv/test_09.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb6HhfSWC6NX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_csv_line(line, n_fields = 9):\n",
        "    defs = [tf.constant(np.nan)] * n_fields\n",
        "    parsed_fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(parsed_fields[0:-1])\n",
        "    y = tf.stack(parsed_fields[-1:])\n",
        "    return x, y\n",
        "\n",
        "def csv_reader_dataset(filenames, n_readers=5,\n",
        "                       batch_size=32, n_parse_threads=5,\n",
        "                       shuffle_buffer_size=10000):\n",
        "    dataset = tf.data.Dataset.list_files(filenames)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
        "        cycle_length = n_readers\n",
        "    )\n",
        "    dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.map(parse_csv_line,\n",
        "                          num_parallel_calls=n_parse_threads)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "batch_size = 32\n",
        "train_set = csv_reader_dataset(train_filenames,\n",
        "                               batch_size = batch_size)\n",
        "valid_set = csv_reader_dataset(valid_filenames,\n",
        "                               batch_size = batch_size)\n",
        "test_set = csv_reader_dataset(test_filenames,\n",
        "                              batch_size = batch_size)\n",
        "# 这样就从csv文件中获取了训练集，验证集，测试集\n",
        "# 接着对这三个文件进行遍历，把这三个文件写到tfrecord中去"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjPrttCMC6Na",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#用来将一组样本转成tf.train.Example格式，并将它序列化\n",
        "def serialize_example(x, y):\n",
        "    \"\"\"Converts x, y to tf.train.Example and serialize\"\"\"\n",
        "    input_feautres = tf.train.FloatList(value = x) #x的类型\n",
        "    label = tf.train.FloatList(value = y) #y的类型\n",
        "    features = tf.train.Features( #构建features\n",
        "        feature = { #构建feature, feature是一个字典，key就是变量的名字，value就是tf.train.Feature\n",
        "            \"input_features\": tf.train.Feature(\n",
        "                float_list = input_feautres),\n",
        "            \"label\": tf.train.Feature(float_list = label) #同理label也是构建一个tf.train.Feature的\n",
        "        }\n",
        "    )\n",
        "    example = tf.train.Example(features = features) #基于features构建example\n",
        "    return example.SerializeToString() #构建完example以后，再去将example序列化\n",
        "    # 这样我们就将x和y转成序列化的example字符串的形式\n",
        "\n",
        "#将从csv文件中读取出来的dataset给转化成tf.exmaple格式，然后在写到tfrecord文件当中去\n",
        "def csv_dataset_to_tfrecords(base_filename, dataset, #base_filename 依然是存成多个文件，所以说有个base_filename,后面还要对他编号\n",
        "                            #dataset 读取出来的dataset\n",
        "                             n_shards, steps_per_shard, #n_shards就是我存成多少个文件\n",
        "                             compression_type = None): #steps_per_shard对于每一个小文件，我该在dataset里走多少步\n",
        "                             #因为我在构建dataset的时候，我们用了repeat,所以dataset里面的遍历是永远也不会结束的\n",
        "                             #所以需要我们自己去算我们需要遍历多少步\n",
        "                             #compression_type我是不是使用某些压缩方法，默认为None\n",
        "    options = tf.io.TFRecordOptions(\n",
        "        compression_type = compression_type) # 定义TFRecordOopptions\n",
        "    all_filenames = []\n",
        "    for shard_id in range(n_shards): #n_shards一共有多少个文件\n",
        "        filename_fullpath = '{}_{:05d}-of-{:05d}'.format(\n",
        "            base_filename, shard_id, n_shards) #具体要生成的小文件的名字\n",
        "        with tf.io.TFRecordWriter(filename_fullpath, options) as writer: #遍历每一个要生成的小文件，打开文件\n",
        "            for x_batch, y_batch in dataset.take(steps_per_shard): #从dataset中取出steps_per_shard个数据，每个数据都是一个batch\n",
        "                for x_example, y_example in zip(x_batch, y_batch): #batch里面的每一条数据\n",
        "                    writer.write(\n",
        "                        serialize_example(x_example, y_example)) #转成tf.exmaple 写到tfrecord文件当中去\n",
        "        all_filenames.append(filename_fullpath) #把所有的文件名都保存下来\n",
        "    return all_filenames #返回所有的文件名\n",
        "    #这个函数生成了一堆小文件，每个小文件都包含了dataset中的一部分，具体的部分，就是从当前的位置取出前steps_per_shard个样本\n",
        "    #因为这个样本都是一个batch，所以说我们把batch中样本都解开，每个样本都给生成它的字符串，然后写到文件当中去"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vI5S0RLC6Nd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#看下是如何被调用的\n",
        "n_shards = 20 #表示需要生成多少个小文件，这里统一把训练集，验证集，测试集都生成20个小文件\n",
        "train_steps_per_shard = 11610 // batch_size // n_shards #steps_per_shard对训练集 验证集 测试集需要走多少步\n",
        "valid_steps_per_shard = 3880 // batch_size // n_shards #valid样本个数为3880，因为我每次取都是从dataset中取batchsize个，所以我要先除以batch_size\n",
        "test_steps_per_shard = 5170 // batch_size // n_shards #因为有n_shards个文件，所以就得到在每个文件上需要多少个batch\n",
        "\n",
        "output_dir = \"generate_tfrecords\" #定义输出文件夹\n",
        "if not os.path.exists(output_dir): #验证它是不是存在，不存在创建一个\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "train_basename = os.path.join(output_dir, \"train\") #定义basename，就是加个train\n",
        "valid_basename = os.path.join(output_dir, \"valid\")\n",
        "test_basename = os.path.join(output_dir, \"test\")\n",
        "\n",
        "train_tfrecord_filenames = csv_dataset_to_tfrecords( #调用上面函数\n",
        "    train_basename, train_set, n_shards, train_steps_per_shard, None)\n",
        "valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    valid_basename, valid_set, n_shards, valid_steps_per_shard, None)\n",
        "test_tfrecord_fielnames = csv_dataset_to_tfrecords(\n",
        "    test_basename, test_set, n_shards, test_steps_per_shard, None)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "711647ec-6a85-459c-a35e-4c82dc5db3c1",
        "id": "Vl3MC0InfTiM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pprint.pprint(train_tfrecord_filenames)\n",
        "pprint.pprint(valid_tfrecord_filenames)\n",
        "pprint.pprint(test_tfrecord_fielnames)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['generate_tfrecords/train_00000-of-00020',\n",
            " 'generate_tfrecords/train_00001-of-00020',\n",
            " 'generate_tfrecords/train_00002-of-00020',\n",
            " 'generate_tfrecords/train_00003-of-00020',\n",
            " 'generate_tfrecords/train_00004-of-00020',\n",
            " 'generate_tfrecords/train_00005-of-00020',\n",
            " 'generate_tfrecords/train_00006-of-00020',\n",
            " 'generate_tfrecords/train_00007-of-00020',\n",
            " 'generate_tfrecords/train_00008-of-00020',\n",
            " 'generate_tfrecords/train_00009-of-00020',\n",
            " 'generate_tfrecords/train_00010-of-00020',\n",
            " 'generate_tfrecords/train_00011-of-00020',\n",
            " 'generate_tfrecords/train_00012-of-00020',\n",
            " 'generate_tfrecords/train_00013-of-00020',\n",
            " 'generate_tfrecords/train_00014-of-00020',\n",
            " 'generate_tfrecords/train_00015-of-00020',\n",
            " 'generate_tfrecords/train_00016-of-00020',\n",
            " 'generate_tfrecords/train_00017-of-00020',\n",
            " 'generate_tfrecords/train_00018-of-00020',\n",
            " 'generate_tfrecords/train_00019-of-00020']\n",
            "['generate_tfrecords/valid_00000-of-00020',\n",
            " 'generate_tfrecords/valid_00001-of-00020',\n",
            " 'generate_tfrecords/valid_00002-of-00020',\n",
            " 'generate_tfrecords/valid_00003-of-00020',\n",
            " 'generate_tfrecords/valid_00004-of-00020',\n",
            " 'generate_tfrecords/valid_00005-of-00020',\n",
            " 'generate_tfrecords/valid_00006-of-00020',\n",
            " 'generate_tfrecords/valid_00007-of-00020',\n",
            " 'generate_tfrecords/valid_00008-of-00020',\n",
            " 'generate_tfrecords/valid_00009-of-00020',\n",
            " 'generate_tfrecords/valid_00010-of-00020',\n",
            " 'generate_tfrecords/valid_00011-of-00020',\n",
            " 'generate_tfrecords/valid_00012-of-00020',\n",
            " 'generate_tfrecords/valid_00013-of-00020',\n",
            " 'generate_tfrecords/valid_00014-of-00020',\n",
            " 'generate_tfrecords/valid_00015-of-00020',\n",
            " 'generate_tfrecords/valid_00016-of-00020',\n",
            " 'generate_tfrecords/valid_00017-of-00020',\n",
            " 'generate_tfrecords/valid_00018-of-00020',\n",
            " 'generate_tfrecords/valid_00019-of-00020']\n",
            "['generate_tfrecords/test_00000-of-00020',\n",
            " 'generate_tfrecords/test_00001-of-00020',\n",
            " 'generate_tfrecords/test_00002-of-00020',\n",
            " 'generate_tfrecords/test_00003-of-00020',\n",
            " 'generate_tfrecords/test_00004-of-00020',\n",
            " 'generate_tfrecords/test_00005-of-00020',\n",
            " 'generate_tfrecords/test_00006-of-00020',\n",
            " 'generate_tfrecords/test_00007-of-00020',\n",
            " 'generate_tfrecords/test_00008-of-00020',\n",
            " 'generate_tfrecords/test_00009-of-00020',\n",
            " 'generate_tfrecords/test_00010-of-00020',\n",
            " 'generate_tfrecords/test_00011-of-00020',\n",
            " 'generate_tfrecords/test_00012-of-00020',\n",
            " 'generate_tfrecords/test_00013-of-00020',\n",
            " 'generate_tfrecords/test_00014-of-00020',\n",
            " 'generate_tfrecords/test_00015-of-00020',\n",
            " 'generate_tfrecords/test_00016-of-00020',\n",
            " 'generate_tfrecords/test_00017-of-00020',\n",
            " 'generate_tfrecords/test_00018-of-00020',\n",
            " 'generate_tfrecords/test_00019-of-00020']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVedtleiC6Nf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#生成压缩后的tfrecord文件\n",
        "n_shards = 20\n",
        "train_steps_per_shard = 11610 // batch_size // n_shards\n",
        "valid_steps_per_shard = 3880 // batch_size // n_shards\n",
        "test_steps_per_shard = 5170 // batch_size // n_shards\n",
        "\n",
        "output_dir = \"generate_tfrecords_zip\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "train_basename = os.path.join(output_dir, \"train\")\n",
        "valid_basename = os.path.join(output_dir, \"valid\")\n",
        "test_basename = os.path.join(output_dir, \"test\")\n",
        "\n",
        "train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    train_basename, train_set, n_shards, train_steps_per_shard,\n",
        "    compression_type = \"GZIP\")\n",
        "valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
        "    valid_basename, valid_set, n_shards, valid_steps_per_shard,\n",
        "    compression_type = \"GZIP\")\n",
        "test_tfrecord_fielnames = csv_dataset_to_tfrecords(\n",
        "    test_basename, test_set, n_shards, test_steps_per_shard,\n",
        "    compression_type = \"GZIP\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhEj02KOC6Nh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a53e46cd-6024-456e-8552-235cc175133b"
      },
      "source": [
        "pprint.pprint(train_tfrecord_filenames)\n",
        "pprint.pprint(valid_tfrecord_filenames)\n",
        "pprint.pprint(test_tfrecord_fielnames)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['generate_tfrecords_zip/train_00000-of-00020',\n",
            " 'generate_tfrecords_zip/train_00001-of-00020',\n",
            " 'generate_tfrecords_zip/train_00002-of-00020',\n",
            " 'generate_tfrecords_zip/train_00003-of-00020',\n",
            " 'generate_tfrecords_zip/train_00004-of-00020',\n",
            " 'generate_tfrecords_zip/train_00005-of-00020',\n",
            " 'generate_tfrecords_zip/train_00006-of-00020',\n",
            " 'generate_tfrecords_zip/train_00007-of-00020',\n",
            " 'generate_tfrecords_zip/train_00008-of-00020',\n",
            " 'generate_tfrecords_zip/train_00009-of-00020',\n",
            " 'generate_tfrecords_zip/train_00010-of-00020',\n",
            " 'generate_tfrecords_zip/train_00011-of-00020',\n",
            " 'generate_tfrecords_zip/train_00012-of-00020',\n",
            " 'generate_tfrecords_zip/train_00013-of-00020',\n",
            " 'generate_tfrecords_zip/train_00014-of-00020',\n",
            " 'generate_tfrecords_zip/train_00015-of-00020',\n",
            " 'generate_tfrecords_zip/train_00016-of-00020',\n",
            " 'generate_tfrecords_zip/train_00017-of-00020',\n",
            " 'generate_tfrecords_zip/train_00018-of-00020',\n",
            " 'generate_tfrecords_zip/train_00019-of-00020']\n",
            "['generate_tfrecords_zip/valid_00000-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00001-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00002-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00003-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00004-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00005-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00006-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00007-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00008-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00009-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00010-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00011-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00012-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00013-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00014-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00015-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00016-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00017-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00018-of-00020',\n",
            " 'generate_tfrecords_zip/valid_00019-of-00020']\n",
            "['generate_tfrecords_zip/test_00000-of-00020',\n",
            " 'generate_tfrecords_zip/test_00001-of-00020',\n",
            " 'generate_tfrecords_zip/test_00002-of-00020',\n",
            " 'generate_tfrecords_zip/test_00003-of-00020',\n",
            " 'generate_tfrecords_zip/test_00004-of-00020',\n",
            " 'generate_tfrecords_zip/test_00005-of-00020',\n",
            " 'generate_tfrecords_zip/test_00006-of-00020',\n",
            " 'generate_tfrecords_zip/test_00007-of-00020',\n",
            " 'generate_tfrecords_zip/test_00008-of-00020',\n",
            " 'generate_tfrecords_zip/test_00009-of-00020',\n",
            " 'generate_tfrecords_zip/test_00010-of-00020',\n",
            " 'generate_tfrecords_zip/test_00011-of-00020',\n",
            " 'generate_tfrecords_zip/test_00012-of-00020',\n",
            " 'generate_tfrecords_zip/test_00013-of-00020',\n",
            " 'generate_tfrecords_zip/test_00014-of-00020',\n",
            " 'generate_tfrecords_zip/test_00015-of-00020',\n",
            " 'generate_tfrecords_zip/test_00016-of-00020',\n",
            " 'generate_tfrecords_zip/test_00017-of-00020',\n",
            " 'generate_tfrecords_zip/test_00018-of-00020',\n",
            " 'generate_tfrecords_zip/test_00019-of-00020']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ8a5vj3C6Nk",
        "colab_type": "code",
        "colab": {},
        "outputId": "7af249d7-4f07-445d-ab4a-b73c6c4e788d"
      },
      "source": [
        "expected_features = {\n",
        "    \"input_features\": tf.io.FixedLenFeature([8], dtype=tf.float32),\n",
        "    \"label\": tf.io.FixedLenFeature([1], dtype=tf.float32)\n",
        "}\n",
        "\n",
        "def parse_example(serialized_example):\n",
        "    example = tf.io.parse_single_example(serialized_example,\n",
        "                                         expected_features)\n",
        "    return example[\"input_features\"], example[\"label\"]\n",
        "\n",
        "def tfrecords_reader_dataset(filenames, n_readers=5,\n",
        "                             batch_size=32, n_parse_threads=5,\n",
        "                             shuffle_buffer_size=10000):\n",
        "    dataset = tf.data.Dataset.list_files(filenames)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filename: tf.data.TFRecordDataset(\n",
        "            filename, compression_type = \"GZIP\"),\n",
        "        cycle_length = n_readers\n",
        "    )\n",
        "    dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.map(parse_example,\n",
        "                          num_parallel_calls=n_parse_threads)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "tfrecords_train = tfrecords_reader_dataset(train_tfrecord_filenames,\n",
        "                                           batch_size = 3)\n",
        "for x_batch, y_batch in tfrecords_train.take(2):\n",
        "    print(x_batch)\n",
        "    print(y_batch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 2.5150437   1.0731637   0.5574401  -0.17273512 -0.6129126  -0.01909157\n",
            "  -0.5710993  -0.02749031]\n",
            " [ 2.5150437   1.0731637   0.5574401  -0.17273512 -0.6129126  -0.01909157\n",
            "  -0.5710993  -0.02749031]\n",
            " [ 2.5150437   1.0731637   0.5574401  -0.17273512 -0.6129126  -0.01909157\n",
            "  -0.5710993  -0.02749031]], shape=(3, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[5.00001]\n",
            " [5.00001]\n",
            " [5.00001]], shape=(3, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 2.5150437   1.0731637   0.5574401  -0.17273512 -0.6129126  -0.01909157\n",
            "  -0.5710993  -0.02749031]\n",
            " [ 2.5150437   1.0731637   0.5574401  -0.17273512 -0.6129126  -0.01909157\n",
            "  -0.5710993  -0.02749031]\n",
            " [ 0.8015443   0.27216142 -0.11624393 -0.20231152 -0.5430516  -0.02103962\n",
            "  -0.5897621  -0.08241846]], shape=(3, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[5.00001]\n",
            " [5.00001]\n",
            " [3.226  ]], shape=(3, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFJZXwLLC6Nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "tfrecords_train_set = tfrecords_reader_dataset(\n",
        "    train_tfrecord_filenames, batch_size = batch_size)\n",
        "tfrecords_valid_set = tfrecords_reader_dataset(\n",
        "    valid_tfrecord_filenames, batch_size = batch_size)\n",
        "tfrecords_test_set = tfrecords_reader_dataset(\n",
        "    test_tfrecord_fielnames, batch_size = batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AogBg5YYC6Np",
        "colab_type": "code",
        "colab": {},
        "outputId": "868cbc0b-eb3f-4a50-9045-8a83b874f094"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation='relu',\n",
        "                       input_shape=[8]),\n",
        "    keras.layers.Dense(1),\n",
        "])\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "callbacks = [keras.callbacks.EarlyStopping(\n",
        "    patience=5, min_delta=1e-2)]\n",
        "\n",
        "history = model.fit(tfrecords_train_set,\n",
        "                    validation_data = tfrecords_valid_set,\n",
        "                    steps_per_epoch = 11160 // batch_size,\n",
        "                    validation_steps = 3870 // batch_size,\n",
        "                    epochs = 100,\n",
        "                    callbacks = callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 1.9641 - val_loss: 1.3548\n",
            "Epoch 2/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.9449 - val_loss: 1.1665\n",
            "Epoch 3/100\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7758 - val_loss: 1.0737\n",
            "Epoch 4/100\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6887 - val_loss: 1.0104\n",
            "Epoch 5/100\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6294 - val_loss: 0.9645\n",
            "Epoch 6/100\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5800 - val_loss: 0.9270\n",
            "Epoch 7/100\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5613 - val_loss: 0.8938\n",
            "Epoch 8/100\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5289 - val_loss: 0.8619\n",
            "Epoch 9/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5253 - val_loss: 0.8352\n",
            "Epoch 10/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5177 - val_loss: 0.8116\n",
            "Epoch 11/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5112 - val_loss: 0.7967\n",
            "Epoch 12/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5143 - val_loss: 0.7788\n",
            "Epoch 13/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4845 - val_loss: 0.7690\n",
            "Epoch 14/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4723 - val_loss: 0.7548\n",
            "Epoch 15/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4560 - val_loss: 0.7447\n",
            "Epoch 16/100\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4514 - val_loss: 0.7349\n",
            "Epoch 17/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4522 - val_loss: 0.7246\n",
            "Epoch 18/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4453 - val_loss: 0.7159\n",
            "Epoch 19/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4397 - val_loss: 0.7070\n",
            "Epoch 20/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4378 - val_loss: 0.7035\n",
            "Epoch 21/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4253 - val_loss: 0.6967\n",
            "Epoch 22/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4309 - val_loss: 0.6929\n",
            "Epoch 23/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4189 - val_loss: 0.6868\n",
            "Epoch 24/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4279 - val_loss: 0.6812\n",
            "Epoch 25/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4243 - val_loss: 0.6752\n",
            "Epoch 26/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4183 - val_loss: 0.6687\n",
            "Epoch 27/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4173 - val_loss: 0.6646\n",
            "Epoch 28/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4117 - val_loss: 0.6624\n",
            "Epoch 29/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4119 - val_loss: 0.6572\n",
            "Epoch 30/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4061 - val_loss: 0.6533\n",
            "Epoch 31/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4131 - val_loss: 0.6507\n",
            "Epoch 32/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4255 - val_loss: 0.6474\n",
            "Epoch 33/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4306 - val_loss: 0.6424\n",
            "Epoch 34/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4362 - val_loss: 0.6386\n",
            "Epoch 35/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4252 - val_loss: 0.6381\n",
            "Epoch 36/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4107 - val_loss: 0.6342\n",
            "Epoch 37/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4054 - val_loss: 0.6320\n",
            "Epoch 38/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3889 - val_loss: 0.6279\n",
            "Epoch 39/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3965 - val_loss: 0.6242\n",
            "Epoch 40/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3928 - val_loss: 0.6198\n",
            "Epoch 41/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3874 - val_loss: 0.6145\n",
            "Epoch 42/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3869 - val_loss: 0.6120\n",
            "Epoch 43/100\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3829 - val_loss: 0.6105\n",
            "Epoch 44/100\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3819 - val_loss: 0.6072\n",
            "Epoch 45/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3755 - val_loss: 0.6042\n",
            "Epoch 46/100\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3775 - val_loss: 0.6026\n",
            "Epoch 47/100\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3814 - val_loss: 0.5990\n",
            "Epoch 48/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3779 - val_loss: 0.5961\n",
            "Epoch 49/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3750 - val_loss: 0.5922\n",
            "Epoch 50/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3766 - val_loss: 0.5925\n",
            "Epoch 51/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3678 - val_loss: 0.5889\n",
            "Epoch 52/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3742 - val_loss: 0.5880\n",
            "Epoch 53/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3649 - val_loss: 0.5849\n",
            "Epoch 54/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3777 - val_loss: 0.5833\n",
            "Epoch 55/100\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3811 - val_loss: 0.5814\n",
            "Epoch 56/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3781 - val_loss: 0.5772\n",
            "Epoch 57/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3866 - val_loss: 0.5766\n",
            "Epoch 58/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3855 - val_loss: 0.5759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TtKdhlLC6Ns",
        "colab_type": "code",
        "colab": {},
        "outputId": "bc6b2d6c-139b-4e03-ed51-24558c7d571f"
      },
      "source": [
        "model.evaluate(tfrecords_test_set, steps = 5160 // batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "161/161 [==============================] - 0s 2ms/step - loss: 0.4429\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4428980045066857"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkVz7PMFC6Nv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}