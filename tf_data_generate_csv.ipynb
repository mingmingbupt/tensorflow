{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "tf_data_generate_csv.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mingmingbupt/tensorflow/blob/master/tf_data_generate_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpuo4SHOAQfl",
        "colab_type": "code",
        "outputId": "c4eceb40-98ee-40fe-8c8e-0efb61057a24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "    print(module.__name__, module.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n",
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
            "matplotlib 3.2.1\n",
            "numpy 1.18.2\n",
            "pandas 1.0.3\n",
            "sklearn 0.22.2.post1\n",
            "tensorflow 2.2.0-rc2\n",
            "tensorflow.keras 2.3.0-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WR0CYgxAQfp",
        "colab_type": "code",
        "outputId": "220d2a2d-c224-4b71-b6d3-83497a8f0a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing = fetch_california_housing()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpVUho0yAQfr",
        "colab_type": "code",
        "outputId": "22305c78-eb56-4e3d-9ece-f647812fa2fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
        "    housing.data, housing.target, random_state = 7)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(\n",
        "    x_train_all, y_train_all, random_state = 11)\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_valid.shape, y_valid.shape)\n",
        "print(x_test.shape, y_test.shape)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11610, 8) (11610,)\n",
            "(3870, 8) (3870,)\n",
            "(5160, 8) (5160,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSk9_pf1AQfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_valid_scaled = scaler.transform(x_valid)\n",
        "x_test_scaled = scaler.transform(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lIJJVRfnZoZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "d65e9dff-3183-4500-ff3d-7475587832d1"
      },
      "source": [
        "housing.feature_names"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MedInc',\n",
              " 'HouseAge',\n",
              " 'AveRooms',\n",
              " 'AveBedrms',\n",
              " 'Population',\n",
              " 'AveOccup',\n",
              " 'Latitude',\n",
              " 'Longitude']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z_M66ESAQfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#我们把scale后的数据呢，生产csv文件\n",
        "output_dir = \"generate_csv\" #定义一个文件夹，把生成的文件放到这个文件夹下\n",
        "if not os.path.exists(output_dir): #如果不存在文件夹的话，就用os.mkdir去创建这个文件夹\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "def save_to_csv(output_dir, data, name_prefix,\n",
        "                header=None, n_parts=10): #把一个单独的dataset,train/valid/test,保存到文件当中去\n",
        "                # output_dir： 输出文件夹，\n",
        "                # data ： dataset\n",
        "                # name_prefix： 因为我们要分别给train/valid/test分别生成csv文件，所以需要name_prefix去做区分\n",
        "                # header： 对于csv文件呢，可能需要一个header,这里默认是None\n",
        "                # n_parts: 我们的datesets是可以把多个文件去merge起来形成一个数据集的，\n",
        "                #      所以说呢，为了测试这个场景，我们需要把我们的数据集去切分成多个文件来进行存储，这里默认是10个文件\n",
        "    path_format = os.path.join(output_dir, \"{}_{:02d}.csv\") # 生成文件名 第一个{}里面天train,test还是valid. 第二个中括号填的是两位的一个整数\n",
        "    # 如果是一位的一个整数，就需要补全一位，这里填每个n_parts的具体数字\n",
        "    filenames = [] #最后会返回所有的文件名\n",
        "    \n",
        "    # enumerate就是给每一组标记一个值，也就是说每一组呢，我们可以通过这个row_indices获得，它标记的值呢，就是file_idx\n",
        "    # 就是一个整数，这个整数可以用来当成一个file_id\n",
        "    for file_idx, row_indices in enumerate(\n",
        "        #我用一个np.arange生成了一个和data一样长度的数组，也就是说我data里面有n个元素，那我们这个数组里面也有n个元素\n",
        "        #它的元素值就是0到n-1，用这个数值当索引。我们再把当索引的这个数值，分成了n_parts个部分\n",
        "        #有了这n_parts部分以后，用每一组里面的索引去data里面去取元素，从而获得数据\n",
        "        np.array_split(np.arange(len(data)), n_parts)):\n",
        "        part_csv = path_format.format(name_prefix, file_idx) #生成对应的子文件名\n",
        "        filenames.append(part_csv)\n",
        "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
        "            if header is not None:\n",
        "                f.write(header + \"\\n\")\n",
        "            for row_index in row_indices: #行索引\n",
        "                f.write(\",\".join(\n",
        "                    [repr(col) for col in data[row_index]])) #用逗号分割，需要做字符串化处理\n",
        "                f.write('\\n')\n",
        "    return filenames\n",
        "\n",
        "train_data = np.c_[x_train_scaled, y_train]  # np.c_把数据按行merge\n",
        "valid_data = np.c_[x_valid_scaled, y_valid]  \n",
        "test_data = np.c_[x_test_scaled, y_test]\n",
        "# 这样就把train valid test数据merge起来了\n",
        "# 然后生成header,从housing数据集获得，因为我们把y也拼接起来了，所以也要加个y的名字MidianHouseValue\n",
        "header_cols = housing.feature_names + [\"MidianHouseValue\"]\n",
        "header_str = \",\".join(header_cols) # 用逗号连接起来\n",
        "\n",
        "train_filenames = save_to_csv(output_dir, train_data, \"train\",\n",
        "                              header_str, n_parts=20) \n",
        "valid_filenames = save_to_csv(output_dir, valid_data, \"valid\",\n",
        "                              header_str, n_parts=10)\n",
        "test_filenames = save_to_csv(output_dir, test_data, \"test\",\n",
        "                             header_str, n_parts=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoGlHefjAQf1",
        "colab_type": "code",
        "outputId": "6c579580-5e08-433b-da19-d91a5c5634dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        }
      },
      "source": [
        "import pprint\n",
        "print(\"train filenames:\")\n",
        "pprint.pprint(train_filenames)\n",
        "print(\"valid filenames:\")\n",
        "pprint.pprint(valid_filenames)\n",
        "print(\"test filenames:\")\n",
        "pprint.pprint(test_filenames)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train filenames:\n",
            "['generate_csv/train_00.csv',\n",
            " 'generate_csv/train_01.csv',\n",
            " 'generate_csv/train_02.csv',\n",
            " 'generate_csv/train_03.csv',\n",
            " 'generate_csv/train_04.csv',\n",
            " 'generate_csv/train_05.csv',\n",
            " 'generate_csv/train_06.csv',\n",
            " 'generate_csv/train_07.csv',\n",
            " 'generate_csv/train_08.csv',\n",
            " 'generate_csv/train_09.csv',\n",
            " 'generate_csv/train_10.csv',\n",
            " 'generate_csv/train_11.csv',\n",
            " 'generate_csv/train_12.csv',\n",
            " 'generate_csv/train_13.csv',\n",
            " 'generate_csv/train_14.csv',\n",
            " 'generate_csv/train_15.csv',\n",
            " 'generate_csv/train_16.csv',\n",
            " 'generate_csv/train_17.csv',\n",
            " 'generate_csv/train_18.csv',\n",
            " 'generate_csv/train_19.csv']\n",
            "valid filenames:\n",
            "['generate_csv/valid_00.csv',\n",
            " 'generate_csv/valid_01.csv',\n",
            " 'generate_csv/valid_02.csv',\n",
            " 'generate_csv/valid_03.csv',\n",
            " 'generate_csv/valid_04.csv',\n",
            " 'generate_csv/valid_05.csv',\n",
            " 'generate_csv/valid_06.csv',\n",
            " 'generate_csv/valid_07.csv',\n",
            " 'generate_csv/valid_08.csv',\n",
            " 'generate_csv/valid_09.csv']\n",
            "test filenames:\n",
            "['generate_csv/test_00.csv',\n",
            " 'generate_csv/test_01.csv',\n",
            " 'generate_csv/test_02.csv',\n",
            " 'generate_csv/test_03.csv',\n",
            " 'generate_csv/test_04.csv',\n",
            " 'generate_csv/test_05.csv',\n",
            " 'generate_csv/test_06.csv',\n",
            " 'generate_csv/test_07.csv',\n",
            " 'generate_csv/test_08.csv',\n",
            " 'generate_csv/test_09.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlVF3R3EAQf3",
        "colab_type": "code",
        "outputId": "3b894663-c1f7-4cd1-9a2a-ea0988703bb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# 1. filename -> dataset\n",
        "# 2. read file -> dataset -> datasets -> merge\n",
        "# 3. parse csv\n",
        "# tensorflow将csv内容读取出来形成dataset。分两步\n",
        "# 1. 把这些filenames生成一个dataset\n",
        "# 2. 对于这个filename组对应的dataset组里面的每一个元素，我们去读取文件，生成一个dataset,\n",
        "#   这样我们就得到多个dataset,也就是dataset组，然后我再把这个dataset组merge起来\n",
        "#   形成一个总的dataset\n",
        "# 其中第一步呢，我们生成一个dataset就可以啦\n",
        "# 第二步呢，我们使用interleave api去进行操作\n",
        "\n",
        "#文件名数据集 这是第一步\n",
        "filename_dataset = tf.data.Dataset.list_files(train_filenames) #list_files是专门处理文件名的，把文件名生成一个dataset\n",
        "for filename in filename_dataset:\n",
        "    print(filename)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'generate_csv/train_18.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_17.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_13.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_15.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_05.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_14.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_02.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_19.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_11.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_12.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_06.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_09.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_03.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_08.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_00.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_07.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_01.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_16.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_04.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'generate_csv/train_10.csv', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvxGLa9MAQf6",
        "colab_type": "code",
        "outputId": "8fa912a6-c8b5-42ee-b754-c0eb258c9a58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "# 第二步，这一步我们使用interleave的api,这个api呢，是dataset的一个函数，这个函数用来遍历dataset里面的每一个元素\n",
        "# 对每一个元素进行操作，最后将操作完的结果合并起来\n",
        "# 第一个参数 依然是一个函数\n",
        "# 第二个参数 \n",
        "n_readers = 5\n",
        "#tf.data.TextLineDataset这个是专门读取文本文件的api，这个api呢，可以把一个文件中对应的文本内容读取出来，生成一个dataset\n",
        "#也就是按行读取文本，生成dataset\n",
        "dataset = filename_dataset.interleave(\n",
        "    lambda filename: tf.data.TextLineDataset(filename).skip(1), # 对每一个filename, 都需要构建一个数据集出来，并用skip(1)把header省略掉\n",
        "    cycle_length = n_readers # 用来控制读取文件的并行度的\n",
        ")\n",
        "#这样我们就把文件集合中的所有文件都读取出来，并将文件内容汇合成一个大的dataset\n",
        "for line in dataset.take(15): #我们看下这个dataset文件内容是什么\n",
        "    print(line.numpy())\n",
        "\n",
        "# 这样我们就得到一个大的dataset,这个大的dataset里面的内容呢，一行就是一个样本的文本\n",
        "# 我们需要把他解析出来"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'0.04326300977263167,-1.0895425985107923,-0.38878716774583305,-0.10789864528874438,-0.6818663605100649,-0.0723871014747467,-0.8883662012710817,0.8213992340186296,1.426'\n",
            "b'-0.8219588176953616,1.874166156711919,0.18212349433218608,-0.03170019246279883,-0.6011178900722581,-0.14337494105109344,1.0852205298015787,-0.8613994495208361,1.054'\n",
            "b'0.42408210084996534,0.9129633171802288,-0.04437481876046234,-0.15297213746739335,-0.24727627804141977,-0.10539166599677323,0.8612674255663844,-1.3357789003702432,3.955'\n",
            "b'0.6303435674178064,1.874166156711919,-0.06713214279531016,-0.12543366804152128,-0.19737553788322462,-0.022722631725889016,-0.692407235065288,0.7265233438487496,2.419'\n",
            "b'0.09734603446040174,0.7527628439249472,-0.20218964416999152,-0.1954700015215477,-0.4060513603629498,0.006785531677655949,-0.813715166526018,0.656614793197258,1.119'\n",
            "b'-0.7543417158936074,-0.9293421252555106,-0.9212720434835953,0.1242806741969112,-0.5983960315181748,-0.18494335623235414,-0.8183808561975836,0.8513600414406984,1.717'\n",
            "b'-0.46794146200516895,-0.9293421252555106,0.11909925912590703,-0.060470113038678074,0.30344643606811583,-0.021851890609536125,1.873722084296329,-1.0411642940532422,1.012'\n",
            "b'0.4369234889778008,-1.9706452014148417,-0.1664210569911193,0.05486205164394496,-0.8379195842775115,-0.1323988058685803,-0.9956770637171147,0.941242463706905,1.73'\n",
            "b'1.6312258686346301,0.3522616607867429,0.04080576110152256,-0.1408895163348976,-0.4632103899987006,-0.06751623819156843,-0.8277122355407183,0.5966931783531273,3.376'\n",
            "b'-1.4803330571456954,-0.6890414153725881,-0.35624704887282904,-0.1725588908792445,-0.8215884329530113,-0.1382309124854157,1.9157132913404298,-1.0211904224385344,0.928'\n",
            "b'0.18702261628258646,-0.20843999560674303,0.005869659830725365,-0.2645340092721605,-0.011381870020860852,-0.015878889894211247,0.05876880205693385,0.17224840654049697,0.84'\n",
            "b'-0.060214068004363165,0.7527628439249472,0.0835940301935345,-0.06250122441959183,-0.03497131082291674,-0.026442380178345683,1.0712234607868782,-1.3707331756959855,1.651'\n",
            "b'-0.49303811681102094,-1.5701440182766375,-0.6933897788607161,0.16277645579446545,0.3279431630548662,-0.08806528786307917,-0.86503775291325,0.6366409215825501,2.033'\n",
            "b'0.04049225382803661,-0.6890414153725881,-0.44379851741607473,0.022374585146687852,-0.22187226486997497,-0.1482850314959248,-0.8883662012710817,0.6366409215825501,2.852'\n",
            "b'0.21174628471128154,1.1532640270631513,-0.2507761334605016,-0.2564987121705146,-0.6473894854916754,0.017590216427099285,0.7959477701644521,-1.1510205879341566,1.935'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "13634a61-e4d8-46d9-c6e5-a36ec7cafdc8",
        "id": "WX8mFNsxzfad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# tf.io.decode_csv(str, record_defaults)\n",
        "# 我们需要把他解析出来，如何解析呢，需要用到tf.io.decode_csv(str, record_defaults)\n",
        "# 这个api第一个参数就是字符串str, 第二个字符串就是record_defaults，用来定义字符串各个field的类型是什么\n",
        "sample_str = '1,2,3,4,5'\n",
        "# record_defaults = [tf.constant(0, dtype=tf.int32)] * 5\n",
        "record_defaults = [#定义的是这5个元素分别的类型，以及他们的默认值，也就是当值有确实的时候，应该设置为什么\n",
        "          #因为是5列，所以record_defaults也是五个元素\n",
        "    tf.constant(0, dtype=tf.int32),  #第一个是tf.contant\n",
        "    0, #可以通过普通的类型，他会自动转换\n",
        "    np.nan,\n",
        "    \"hello\",\n",
        "    tf.constant([]) #无固定类型，默认是float32\n",
        "]\n",
        "\n",
        "#其实1还可以被表示为int float string 等等，\n",
        "parsed_fields = tf.io.decode_csv(sample_str, record_defaults)\n",
        "print(parsed_fields)\n",
        "#"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=float32, numpy=3.0>, <tf.Tensor: shape=(), dtype=string, numpy=b'4'>, <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSw-VQMyAQgB",
        "colab_type": "code",
        "outputId": "07e0d263-e4b9-449c-a3d5-096637f70297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "    parsed_fields = tf.io.decode_csv(',,,,', record_defaults) #如果传进来的字符串是一个不对字符串，会报什么错\n",
        "except tf.errors.InvalidArgumentError as ex:\n",
        "    print(ex)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVq7PJgRAQgD",
        "colab_type": "code",
        "outputId": "f8a9850e-630f-44b1-df6f-87479a193caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "    parsed_fields = tf.io.decode_csv('1,2,3,4,5,6,7', record_defaults)\n",
        "except tf.errors.InvalidArgumentError as ex:\n",
        "    print(ex)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expect 5 fields but have 7 in record 0 [Op:DecodeCSV]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuLhCR2vAQgG",
        "colab_type": "code",
        "outputId": "1dcda81f-27c0-4ed4-d3a0-4762a70a6952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "#把csv的一行解析出来，第一个参数就是输入的str,第二个参数就是要解析出来多少个field\n",
        "def parse_csv_line(line, n_fields = 9):\n",
        "    defs = [tf.constant(np.nan)] * n_fields #就是我们的record_defaults，是具有n_fields的float值，\n",
        "    parsed_fields = tf.io.decode_csv(line, record_defaults=defs) # 用tf.io.decode_csv把字符串解析出来\n",
        "    x = tf.stack(parsed_fields[0:-1]) # 每一行里有n_fields=9个元素，其中前8个元素是x，最后一个元素是y\n",
        "    y = tf.stack(parsed_fields[-1:]) # 我需要把前8个值变成一个向量，第9个值变成一个向量。变成向量需要tf.stack进行转换\n",
        "    return x, y\n",
        "\n",
        "parse_csv_line(b'-0.9868720801669367,0.832863080552588,-0.18684708416901633,-0.14888949288707784,-0.4532302419670616,-0.11504995754593579,1.6730974284189664,-0.7465496877362412,1.138',\n",
        "               n_fields=9)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
              " array([-0.9868721 ,  0.8328631 , -0.18684709, -0.1488895 , -0.45323023,\n",
              "        -0.11504996,  1.6730974 , -0.74654967], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.138], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ues_A7zXAQgJ",
        "colab_type": "code",
        "outputId": "c91c8681-c7f5-494d-cd74-b632798c506a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "# 1. filename -> dataset\n",
        "# 2. read file -> dataset -> datasets -> merge\n",
        "# 3. parse csv\n",
        "# 将csv文件读取出来，形成dataset完整过程\n",
        "# filenames 读取文件名\n",
        "# n_readers 并行程度\n",
        "# n_parse_threads 我们在parses时候呢，并行程度是多少，是用来做解析的时候，并行度是多少\n",
        "# shuffle_buffer_size shuffle的时候我们的buffer是多少，这里设成10000，是我用来混排数据的时候，我的buffer的size是多大\n",
        "def csv_reader_dataset(filenames, n_readers=5,\n",
        "                       batch_size=32, n_parse_threads=5,\n",
        "                       shuffle_buffer_size=10000):\n",
        "    dataset = tf.data.Dataset.list_files(filenames) #1.把filename变成一个dataset\n",
        "    dataset = dataset.repeat() #做repeat,数据集重复多少次，如果不加参数，就是重复无限次\n",
        "    dataset = dataset.interleave( #文件名转换成文本内容\n",
        "        lambda filename: tf.data.TextLineDataset(filename).skip(1), #忽略header第一行\n",
        "        cycle_length = n_readers\n",
        "    )\n",
        "    dataset.shuffle(shuffle_buffer_size) #做shuffle,即我把数据做混排，shuffle需要shuffle_buffer_size，即shuffle需要的内存有多大\n",
        "    dataset = dataset.map(parse_csv_line, #将dataset每一个元素做个操作，将返回的结果组成一个新的dataset\n",
        "                          num_parallel_calls=n_parse_threads) #num_parallel_calls 并行程度\n",
        "    #我们发现map和interleave做的事情很相似，interleave做的事情是对于原来dataset里面的每一个元素，都会生成一个新的dataset,再把这些\n",
        "    #新的dataset合并，生成一个大的dataset。\n",
        "    dataset = dataset.batch(batch_size) #做batch操作\n",
        "    return dataset #返回dataset\n",
        "\n",
        "train_set = csv_reader_dataset(train_filenames, batch_size=3)\n",
        "for x_batch, y_batch in train_set.take(2):\n",
        "    print(\"x:\")\n",
        "    pprint.pprint(x_batch)\n",
        "    print(\"y:\")\n",
        "    pprint.pprint(y_batch)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "<tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n",
            "array([[ 0.48530516, -0.8492419 , -0.06530126, -0.02337966,  1.4974351 ,\n",
            "        -0.07790658, -0.90236324,  0.78145146],\n",
            "       [-1.119975  , -1.3298433 ,  0.14190045,  0.4658137 , -0.10301778,\n",
            "        -0.10744184, -0.7950524 ,  1.5304717 ],\n",
            "       [ 0.63034356,  1.8741661 , -0.06713215, -0.12543367, -0.19737554,\n",
            "        -0.02272263, -0.69240725,  0.72652334]], dtype=float32)>\n",
            "y:\n",
            "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
            "array([[2.956],\n",
            "       [0.66 ],\n",
            "       [2.419]], dtype=float32)>\n",
            "x:\n",
            "<tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n",
            "array([[ 0.09734604,  0.75276285, -0.20218964, -0.19547   , -0.40605137,\n",
            "         0.00678553, -0.81371516,  0.6566148 ],\n",
            "       [-0.66722274, -0.04823952,  0.34529406,  0.53826684,  1.8521839 ,\n",
            "        -0.06112538, -0.8417093 ,  1.5204847 ],\n",
            "       [-0.7432054 ,  0.91296333, -0.64432025, -0.1479097 ,  0.7398511 ,\n",
            "         0.11427691, -0.7950524 ,  0.68158215]], dtype=float32)>\n",
            "y:\n",
            "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
            "array([[1.119],\n",
            "       [1.59 ],\n",
            "       [1.438]], dtype=float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyL8TBX0AQgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "train_set = csv_reader_dataset(train_filenames,\n",
        "                               batch_size = batch_size)\n",
        "valid_set = csv_reader_dataset(valid_filenames,\n",
        "                               batch_size = batch_size)\n",
        "test_set = csv_reader_dataset(test_filenames,\n",
        "                              batch_size = batch_size)\n",
        "#训练集 测试集 验证集都取出来"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M3Nou_PAQgN",
        "colab_type": "code",
        "outputId": "39c1fa8f-16c9-465b-d5c4-7557e34222f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation='relu',\n",
        "                       input_shape=[8]),\n",
        "    keras.layers.Dense(1),\n",
        "])\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "callbacks = [keras.callbacks.EarlyStopping(\n",
        "    patience=5, min_delta=1e-2)]\n",
        "\n",
        "#x和y都在train_set里\n",
        "history = model.fit(train_set,\n",
        "                    validation_data = valid_set,\n",
        "                    steps_per_epoch = 11160 // batch_size, #因为我们的dataset是不停的去产生数据的，所以说我们的fit函数不知道跑多少次\n",
        "                    #这里就用11160（就是我们train_set的大小），除以batch_size呢，就是遍历一次需要多少次迭代\n",
        "                    validation_steps = 3870 // batch_size, #同理我的valid data也需要这样一个值\n",
        "                    epochs = 100,\n",
        "                    callbacks = callbacks)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 1.6179 - val_loss: 5.1179\n",
            "Epoch 2/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5217 - val_loss: 0.4282\n",
            "Epoch 3/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4083 - val_loss: 0.3994\n",
            "Epoch 4/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3815 - val_loss: 0.3828\n",
            "Epoch 5/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3711 - val_loss: 0.3848\n",
            "Epoch 6/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3588 - val_loss: 0.3776\n",
            "Epoch 7/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3828 - val_loss: 0.3825\n",
            "Epoch 8/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3644 - val_loss: 0.3730\n",
            "Epoch 9/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3521 - val_loss: 0.3672\n",
            "Epoch 10/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3613 - val_loss: 0.3689\n",
            "Epoch 11/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3510 - val_loss: 0.3726\n",
            "Epoch 12/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3695 - val_loss: 0.3615\n",
            "Epoch 13/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3530 - val_loss: 0.3594\n",
            "Epoch 14/100\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3409 - val_loss: 0.3638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWT__acIAQgP",
        "colab_type": "code",
        "outputId": "2ec0bbbf-2d4c-4c08-a4bc-ff2b5c8cc5cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "model.evaluate(test_set, steps = 5160 // batch_size) #因为dataset是不停产生数据集的，所以也需要指定step,用test_size=5160,除以batch_size"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "161/161 [==============================] - 0s 2ms/step - loss: 0.3664\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3664287328720093"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNHcjRU9AQgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}