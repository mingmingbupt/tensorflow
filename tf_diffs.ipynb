{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "tf_diffs.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mingmingbupt/tensorflow/blob/master/tf_diffs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3S6PHbFKBOY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "82397dc7-72a4-401f-e1bd-d096bd55a4b3"
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "    print(module.__name__, module.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc1\n",
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
            "matplotlib 3.2.1\n",
            "numpy 1.18.2\n",
            "pandas 0.25.3\n",
            "sklearn 0.22.2.post1\n",
            "tensorflow 2.2.0-rc1\n",
            "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF4swGjDKBOb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3cbfc9a1-e55a-499a-b4bd-cee15b93bf54"
      },
      "source": [
        "# 求导数例子\n",
        "def f(x):\n",
        "    return 3. * x ** 2 + 2. * x - 1\n",
        "#这里定义一个近似求导数方法，f是函数，x是指在哪一点上的导数，eps足够小的时候，导数就足够真\n",
        "def approximate_derivative(f, x, eps=1e-3):\n",
        "    return (f(x + eps) - f(x - eps)) / (2. * eps)\n",
        "\n",
        "print(approximate_derivative(f, 1.))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.999999999999119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhn3zbe0KBOe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9abc9166-84db-41fe-eba7-6e66aa59f562"
      },
      "source": [
        "def g(x1, x2): # 有两个变量，如何求到苏呢\n",
        "    return (x1 + 5) * (x2 ** 2)\n",
        "\n",
        "def approximate_gradient(g, x1, x2, eps=1e-3): #这是一个多元函数，\n",
        "    dg_x1 = approximate_derivative(lambda x: g(x, x2), x1, eps) #x1求偏导，把x2固定住\n",
        "    dg_x2 = approximate_derivative(lambda x: g(x1, x), x2, eps) #x2求偏导，把x1固定住\n",
        "    return dg_x1, dg_x2\n",
        "\n",
        "print(approximate_gradient(g, 2., 3.))\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8.999999999993236, 41.999999999994486)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sthb6PRaKBOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "983663b0-8340-4023-d218-f63f201e487f"
      },
      "source": [
        "#下面看下在tensorflow里是如何自定义实现导数\n",
        "x1 = tf.Variable(2.0)\n",
        "x2 = tf.Variable(3.0)\n",
        "\n",
        "#在tensorflow里面求导数，需要用到一个叫tf.GradientTape()的东西，这个东西呢，就可以帮我们进行导数的求解\n",
        "with tf.GradientTape() as tape:\n",
        "    # 我打开tf.GradientTape()这个东西之后呢，就可以去定义我们的函数了\n",
        "    z = g(x1, x2) #这里用的是g这个函数，也就是我把x1,x2传给这个g函数，然后得到一个z\n",
        "    # 定义好这个函数呢，我们就可以用tape去帮我们求偏导\n",
        "\n",
        "#下面我们用tape 来求z对于x1的偏导\n",
        "dz_x1 = tape.gradient(z, x1) #第一个参数是函数的输出，第二个参数是他指定的变量，也就是他对那个变量求偏导\n",
        "print(dz_x1)\n",
        "\n",
        "try:\n",
        "    dz_x2 = tape.gradient(z, x2) #我们也可以用tape去做z对x2的偏导，但是在tensorflow实现里面呢，这个tape是只能用一次的。\n",
        "    #也就是这个tape调用了一次以后呢，这个对象就会被释放，就不能再用了，所以我们先用try catch呢把异常抓住，后面再看如何解决这个问题\n",
        "except RuntimeError as ex:\n",
        "    print(ex)\n",
        "#从结果中看出，z对于x1的偏导呢，这个值是9\n",
        "#对于z对于x2求导呢，我们发现他会报出一个异常，这个异常说明，对于没有被保存的tapes, 不能被执行第二次，只能执行一次\n",
        "#下面看下如何解决这个问题"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(9.0, shape=(), dtype=float32)\n",
            "GradientTape.gradient can only be called once on non-persistent tapes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM5nBrBPKBOi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8a48c6e7-359f-472d-bc07-622e97d4c7a9"
      },
      "source": [
        "#因为这是一个没有保存的tapes,所以要把它声明称一个可以保存的tapes，persistent = True\n",
        "#这样就可以对这个tape调用多次，但是呢我们要注意，这个tape呢，用完以后要删掉 del tape\n",
        "#因为这里呢把persistent设成true了，系统就不会帮你去释放资源了，你需要自己去释放资源\n",
        "x1 = tf.Variable(2.0)\n",
        "x2 = tf.Variable(3.0)\n",
        "with tf.GradientTape(persistent = True) as tape:\n",
        "    z = g(x1, x2)\n",
        "\n",
        "dz_x1 = tape.gradient(z, x1)\n",
        "dz_x2 = tape.gradient(z, x2)\n",
        "print(dz_x1, dz_x2)\n",
        "\n",
        "del tape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(9.0, shape=(), dtype=float32) tf.Tensor(42.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuwQmd6UKBOk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "866d96ec-88b5-4009-aeab-df142518388d"
      },
      "source": [
        "#虽然上面这样，我们可以求出z对于x1 x2的偏导，但是我们毕竟是求了两次\n",
        "#我们还有两外一种方法，我们可以同时把z关于x1和x2的偏导都求出来\n",
        "#下面演示该如何实现\n",
        "x1 = tf.Variable(2.0)\n",
        "x2 = tf.Variable(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "    z = g(x1, x2)\n",
        "\n",
        "dz_x1x2 = tape.gradient(z, [x1, x2]) #这里传入的是[x1,x2], 输出的是dz_x1x2\n",
        "\n",
        "print(dz_x1x2) #这样他返回的偏导呢 也是一个列表，分别是关于x1和x2得偏导"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: shape=(), dtype=float32, numpy=9.0>, <tf.Tensor: shape=(), dtype=float32, numpy=42.0>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh2dZV7EKBOn",
        "colab_type": "code",
        "colab": {},
        "outputId": "60c2d37e-a61b-4885-f924-31bdca439330"
      },
      "source": [
        "#上面是tf.GradientTape对tensorflow里面变量做偏导\n",
        "#下面试验下 如何对常量求偏导\n",
        "x1 = tf.constant(2.0)\n",
        "x2 = tf.constant(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "    z = g(x1, x2)\n",
        "\n",
        "dz_x1x2 = tape.gradient(z, [x1, x2])\n",
        "\n",
        "print(dz_x1x2)\n",
        "#我们发现我们得到得都是None\n",
        "#那么就完全没办法去关注这个constant得梯度了吗\n",
        "#不是的，在tensorflow里可以通过一些变动呢去关注这个constant的导数\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, None]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZWa9Kk_KBOq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1fc194bb-f2fc-4e8a-8bc3-a433be627d26"
      },
      "source": [
        "#那么是什么样的变动呢\n",
        "#在这里呢，需要做的呢，是我要告诉这个tape呢，我需要关注哪些constant\n",
        "x1 = tf.constant(2.0)\n",
        "x2 = tf.constant(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(x1) #就是我要关注这两个常量的导数\n",
        "    tape.watch(x2) #就是我要关注这两个常量的导数，虽然我可能不去计算他，但是呢，还是要关注下\n",
        "    z = g(x1, x2) \n",
        "\n",
        "dz_x1x2 = tape.gradient(z, [x1, x2])\n",
        "\n",
        "print(dz_x1x2)\n",
        "#这样就可以得到常量的导数了"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: shape=(), dtype=float32, numpy=9.0>, <tf.Tensor: shape=(), dtype=float32, numpy=42.0>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nviXaBldKBOs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9b910f00-8552-44d8-ef9a-f524176da814"
      },
      "source": [
        "#这是由多个目标函数下，多个目标函数对一个变量求导的实现\n",
        "#在上面呢，我们用一个目标函数给两个变量求导数，那能不能我有两个目标函数，然后对一个变量求导数呢\n",
        "#下面看如何实现\n",
        "x = tf.Variable(5.0) #定义一个变量，值为5\n",
        "with tf.GradientTape() as tape: \n",
        "    z1 = 3 * x  #定义两个函数z1\n",
        "    z2 = x ** 2 #定义两个函数z2\n",
        "tape.gradient([z1, z2], x) #用tape求z1,z2对x的导数\n",
        "#得到的结果是 z1对于x导数加上z2对于x的导数\n",
        "#这里z1对x的导数应该是3，z2对x的导数应该是10，合起来就是13\n",
        "#这是由多个目标函数下，多个目标函数对一个变量求导的实现"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=13.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGjl0VxnKBOw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7cb54188-a02e-46f0-9449-41117972d12d"
      },
      "source": [
        "#如何求解二阶导数呢\n",
        "#需要使用一个嵌套的tape\n",
        "#这里依然使用g作为我们的目标函数来进行求导\n",
        "x1 = tf.Variable(2.0)\n",
        "x2 = tf.Variable(3.0)\n",
        "with tf.GradientTape(persistent=True) as outer_tape: #外层tape,这两个tape都可能调用多次，所以都用了persistent=True\n",
        "    with tf.GradientTape(persistent=True) as inner_tape: #内层tape,这两个tape都可能调用多次，所以都用了persistent=True\n",
        "        z = g(x1, x2) #这里定义目标函数\n",
        "    #在嵌套的内部，就可以求inner_tape的梯度了\n",
        "    inner_grads = inner_tape.gradient(z, [x1, x2])  #求z对[x1, x2]的偏导\n",
        "\n",
        "outer_grads = [outer_tape.gradient(inner_grad, [x1, x2]) #到了外层以后呢，我们分别用inner_grad内层求出来的导数呢去对x1,x2求偏导\n",
        "               for inner_grad in inner_grads]\n",
        "\n",
        "print(outer_grads) #把外层求导的结果打印出来\n",
        "del inner_tape\n",
        "del outer_tape\n",
        "#得到的是一个2*2的矩阵，\n",
        "#分别是z对于x1的二阶导数，z先对x2求导再对x1求导\n",
        "#然后呢右下角的值正好是反过来\n",
        "#z先对x1求导，再对x2求导 和 z对x2的二阶导数\n",
        "\n",
        "#如果想求三阶 四阶导数的话，不停的嵌套就可以了，但是平时呢，一般也就求到二阶\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[None, <tf.Tensor: shape=(), dtype=float32, numpy=6.0>], [<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, <tf.Tensor: shape=(), dtype=float32, numpy=14.0>]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z_P-vtdKBOy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "43d1311f-597a-457a-c929-7f6e242c6112"
      },
      "source": [
        "#下面模拟实现 梯度下降算法\n",
        "learning_rate = 0.1\n",
        "x = tf.Variable(0.0)\n",
        "\n",
        "for _ in range(100): #进行100步，每一步都求导，然后更新\n",
        "    with tf.GradientTape() as tape: #用GradientTape去进行求导\n",
        "        z = f(x) #f(x)作为目标函数\n",
        "    dz_dx = tape.gradient(z, x) #求导\n",
        "    x.assign_sub(learning_rate * dz_dx) #x =x -learning_rate*导数\n",
        "print(x)\n",
        "#这是一个简单的梯度下降的模拟"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abnb7UWOKBO0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3f7e905-79cb-4241-9658-e0a3c6930541"
      },
      "source": [
        "#keras里面用的是optimizer，那么如何和optimizer结合使用呢\n",
        "#将GradientTape如何和optimizer结合使用\n",
        "#这里依然将f(x)设为目标函数\n",
        "learning_rate = 0.1\n",
        "x = tf.Variable(0.0)\n",
        "\n",
        "#这里需要使用optimizer.apply_gradients来更新我们的x\n",
        "optimizer = keras.optimizers.SGD(lr = learning_rate)\n",
        "\n",
        "for _ in range(100):\n",
        "    with tf.GradientTape() as tape:\n",
        "        z = f(x)\n",
        "    dz_dx = tape.gradient(z, x)\n",
        "    optimizer.apply_gradients([(dz_dx, x)]) #这里只需要调用apply_gradients就可以啦\n",
        "    #这里面参数呢，是一个列表，列表里面呢，每个元素都是一个pair.pair里面存储的就是一个变量，以及他的梯度\n",
        "    #梯度在前，变量在后，在这里呢，我们只有一个变量，所以列表里只有一个元素\n",
        "print(x)\n",
        "#大家只要能定义胡目标函数，就可以用tf.GradientTape()去进行求导了"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCpx1ZyRKBO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}